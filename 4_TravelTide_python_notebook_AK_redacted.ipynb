{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DA202 · Customer Segmentation: Mastery Project\n",
    "* Author: Alan Kong @[LinkedIn](https://www.linkedin.com/in/alan-kong-professional/)\n",
    "* Dashboard @[Tableau Public](https://public.tableau.com/views/5_Dashboard_TravelTide_AKclean/Dash?:language=en-US&:display_count=n&:origin=viz_share_link)\n",
    "* To replicate the analysis [data.csv](https://github.com/coderedstorage/TravelTide/commit/e858a92d61a1c334054415c7c8b1edb88e48f7e7) is available on GitHub. Please skip Steps B-D in the python notebook as required information to run those sections are too large and not provided on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import psycopg2\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "from geopy.distance import geodesic\n",
    "from geopy.distance import great_circle\n",
    "from geographiclib.geodesic import Geodesic\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandas.tseries.offsets import MonthBegin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pre-set paths and check the directories exist and write-able"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All directories exist with write permissions\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = r\"C:\\\\data\\\\tt\\\\py\\\\\"                   # preset destinations for output\n",
    "\n",
    "CONFIG_PATH = {k: f\"{BASE_PATH}{v}\" for k, v in {\n",
    "    \"full\": \"full.csv\", \"data\": \"data.csv\", \n",
    "    \"precut\": \"precut.csv\", \"clean\": \"clean.csv\", \"outliers\": \"outliers.csv\", # transaction level data\n",
    "    \"user\": \"user.csv\", \"selection\": \"selection.csv\", # user level data\n",
    "    \"corr_matrix\": \"corr_matrix.csv\", \"sample_size_matrix\": \"sample_size_matrix.csv\", # user level data\n",
    "    \"kmeans_t\": \"kmeans_t.csv\", \"kmeans_u\": \"kmeans_u.csv\",                 # kmeans\n",
    "    \"dummy\": \"dummy.csv\" # dummy\n",
    "}.items()}\n",
    "CONFIG_PATH[\"chromedriver.exe\"] = r\"C:\\\\scrapper\\\\chromedriver.exe\"\n",
    "\n",
    "def check_directories():                            # function to check preset destinations for output\n",
    "    all_good = True  # Initialize flag for directory status\n",
    "    for key, path in CONFIG_PATH.items():\n",
    "        directory = os.path.dirname(path)\n",
    "        msg = f\"Problem with {key} [{path}]\\n\"\n",
    "        \n",
    "        if not os.path.exists(directory) or not os.access(directory, os.W_OK):\n",
    "            all_good = False  # Set flag to False\n",
    "            if not os.path.exists(directory):\n",
    "                msg += \"• Directory doesn't exist. Creating now...\\n\"\n",
    "                os.makedirs(directory)\n",
    "            else:\n",
    "                msg += \"• Directory exists but has no write permissions\\n\"\n",
    "            print(msg)  # Print the message and adds an empty line\n",
    "    if all_good:\n",
    "        print(\"All directories exist with write permissions\")\n",
    "    return all_good\n",
    "\n",
    "all_good = check_directories() # call the function\n",
    "# All directories exist with write permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Practical code snippets for adhoc use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>col1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>col2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>col3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>col4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lat1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lon1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lat2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lon2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cancellation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>flight_booked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>return_flight_booked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>clicks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               variables\n",
       "0                     id\n",
       "1                   col1\n",
       "2                   col2\n",
       "3                   col3\n",
       "4                   col4\n",
       "5                   lat1\n",
       "6                   lon1\n",
       "7                   lat2\n",
       "8                   lon2\n",
       "9           cancellation\n",
       "10         flight_booked\n",
       "11  return_flight_booked\n",
       "12                clicks"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = pd.DataFrame({                                                      # dummy df to test some of the snippets\n",
    "    'id': ['r1', 'r2', 'r3', 'r4', 'r5', 'r6'],\n",
    "    'col1': [100, 200, 150, np.nan, 180, 220],\n",
    "    'col2': [0.5, 0.2, 0.1, np.nan, 0.25, 0.15],\n",
    "    'col3': ['Hilton', 'InterContinental', np.nan, 'Mariott', 'Banyan Tree', 'Ramada'],\n",
    "    'col4': ['Houston', 'SF', 'SF', np.nan, 'Miami', 'LA'],\n",
    "    'lat1': [f\"{lat:.4f}\" for lat in [40.7128, 34.0522, 50.7128, 45.0522, 40.7128, 35.0522]],\n",
    "    'lon1': [f\"{lat:.4f}\" for lat in [50.7128, 44.0522, 60.7128, 54.0522, 60.7128, 44.0522]],\n",
    "    'lat2': [f\"{lat:.4f}\" for lat in [35.7128, 25.0522, 5.7128, 35.0522, 44.7128, 55.0522]],\n",
    "    'lon2': [f\"{lat:.4f}\" for lat in [25.7128, 30.0522, 35.7128, 45.0522, 50.7128, 58.0522]],    \n",
    "    'cancellation': [False, False, True, np.nan, False, True],\n",
    "    'flight_booked': [True, False, True, np.nan, False, True],\n",
    "    'return_flight_booked': [True, False, np.nan, np.nan, False, False],\n",
    "    'clicks': ['12.0 ', ' 555', '568.0', ' 65.0  ', ' 7.0 ', ' 5.0']})\n",
    "pd.DataFrame(list(dummy.columns), columns=['variables'])                    # convert a list to df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Call without assigning df, automatically modifies original df in arg: def(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df name is dummy ==> 5 col1 , 5 col2 , 5 col3\n"
     ]
    }
   ],
   "source": [
    "def df_name_and_variable_sizes(df, col_list):                                           # output name of df and count unique values of its variables\n",
    "    def find_df_name(df):                                                               # find df name\n",
    "        name = [x for x in globals() if globals()[x] is df][0]\n",
    "        print(f\"df name is {name} \", end=\"\")                                            # end=\"\" arg so next print is in same line\n",
    "    find_df_name(df)                                                                    # print df name\n",
    "    unique_counts = {col: df[col].nunique() for col in col_list}\n",
    "    print(\"==> \"+ \" , \".join(f\"{unique_counts[col]} {col}\" for col in col_list))        # print counts\n",
    "df_name_and_variable_sizes(dummy, ['col1', 'col2', 'col3'])                                    # call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flight_booked</th>\n",
       "      <th>return_flight_booked</th>\n",
       "      <th>distance_method</th>\n",
       "      <th>flown_km</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>flight_category</th>\n",
       "      <th>flight_long_haul</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>karney</td>\n",
       "      <td>4500.501108</td>\n",
       "      <td>2250.250554</td>\n",
       "      <td>medium haul</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>karney</td>\n",
       "      <td>5503.569008</td>\n",
       "      <td>5503.569008</td>\n",
       "      <td>long haul</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>karney</td>\n",
       "      <td>2470.518028</td>\n",
       "      <td>2470.518028</td>\n",
       "      <td>medium haul</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  flight_booked return_flight_booked distance_method     flown_km  \\\n",
       "0          True                 True          karney  4500.501108   \n",
       "1         False                False             NaN          NaN   \n",
       "2          True                  NaN          karney  5503.569008   \n",
       "3           NaN                  NaN             NaN          NaN   \n",
       "4         False                False             NaN          NaN   \n",
       "5          True                False          karney  2470.518028   \n",
       "\n",
       "   distance_km flight_category flight_long_haul  \n",
       "0  2250.250554     medium haul            False  \n",
       "1          NaN             NaN              NaN  \n",
       "2  5503.569008       long haul             True  \n",
       "3          NaN             NaN              NaN  \n",
       "4          NaN             NaN              NaN  \n",
       "5  2470.518028     medium haul            False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate flown distances, 1-way flight equivalent distance needs to be separately calculated per choice of flown distances\n",
    "# lat1_col='home_airport_lat', lon1_col='home_airport_lon', lat2_col='destination_airport_lat', lon2_col='destination_airport_lon', flight_booked_col='flight_booked', return_flight_col='return_flight_booked', chosen_method='karney'\n",
    "# method for flown_km and distance_km for metrics are 'haversine_basic', 'haversine_oblate', 'karney'\n",
    "def distances(df, lat1_col='home_airport_lat', lon1_col='home_airport_lon', lat2_col='destination_airport_lat', lon2_col='destination_airport_lon',\n",
    "              flight_booked_col='flight_booked', return_flight_col='return_flight_booked', method='karney', \n",
    "              short_haul_max=1287, medium_haul_max=4023, long_haul_max=10460):\n",
    "    \"\"\"\n",
    "    * Haversine: Great-circle distance between two points on surface of a perfect sphere; (oblate==True) to adjust for Earth's oblateness (use weighted average of equatorial and polar radii based on latitudes of two points) \n",
    "    * Charles Karney's Method: Use geographiclib library. Geodesic.WGS84.Inverse method from geographiclib calculates distance between two points based on WGS-84 ellipsoidal model of the Earth. Recent and more accurate method for geodetic computations than adjusted haversine. Accurate results for nearly antipodal points. superior to previously commonly used Vincenty's method that might not converge\n",
    "    \"\"\"    \n",
    "    valid_methods = ['haversine_basic', 'haversine_oblate', 'karney']\n",
    "    if method not in valid_methods and method is not None:\n",
    "        print(f\"Invalid method chosen: {method}. Choose from {', '.join(valid_methods)}.\")\n",
    "        return\n",
    "\n",
    "    def is_true(val):                                                                                                   # check if a value is true-like\n",
    "        return val in [True, 'True', 'true', 1]\n",
    "    \n",
    "    def is_false_or_nan(val):                                                                                           # check if a value is false or NaN\n",
    "        return val in [False, 'False', 'false', 0, 'NaN', 'nan', np.nan]\n",
    "    \n",
    "    for col in [lat1_col, lon1_col, lat2_col, lon2_col]:                                                                # ensure columns are of float type\n",
    "        df[col] = df[col].astype(float)\n",
    "    \n",
    "    def haversine(lat1, lon1, lat2, lon2, oblate=False):                                                                # haversine estimate distances\n",
    "        lat1, lon1, lat2, lon2 = map(np.radians, map(float, [lat1, lon1, lat2, lon2]))\n",
    "        R = 6371 if not oblate else (1 - 1/298.257223563) * 6378.137 / np.sqrt(1 - (1/298.257223563)**2 * np.sin((lat1 + lat2) / 2)**2)\n",
    "        dLat, dLon = lat2 - lat1, lon2 - lon1\n",
    "        a = np.sin(dLat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dLon / 2)**2\n",
    "        return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a)) \n",
    "    \n",
    "    def categorize(distance):\n",
    "        if math.isnan(distance) or distance == 0:\n",
    "            return None, None\n",
    "        flight_category = ('short haul' if distance <= short_haul_max else\n",
    "                           'medium haul' if distance <= medium_haul_max else \n",
    "                           'long haul' if distance <= long_haul_max else 'ultra long haul')\n",
    "        flight_long_haul = distance > medium_haul_max\n",
    "        return flight_category, flight_long_haul\n",
    "    \n",
    "    def calculate_all(row):                                                                                             # main function\n",
    "        if is_false_or_nan(row[flight_booked_col]):                                                                     # not accounting for cancellation, such that theoretical flight distances still calculated\n",
    "            return [np.nan]*8\n",
    "        lat1, lon1, lat2, lon2 = row[lat1_col], row[lon1_col], row[lat2_col], row[lon2_col]\n",
    "        h_basic, h_oblate = haversine(lat1, lon1, lat2, lon2), haversine(lat1, lon1, lat2, lon2, oblate=True)           # call the function for haversine both without and with oblate adjustment\n",
    "        dist_karney = Geodesic.WGS84.Inverse(lat1, lon1, lat2, lon2)['s12'] / 1000                                      # charles karney's method, conver to km\n",
    "        multiplier = 2 if (row[return_flight_col]==True) else 1                                                                 # multiplier for return flight\n",
    "    \n",
    "        distance_km = {'haversine_basic': h_basic, 'haversine_oblate': h_oblate, 'karney': dist_karney}.get(method, 0)  # method is drawn from the keys of dict\n",
    "        flown_km = distance_km * multiplier\n",
    "\n",
    "        flight_category, flight_long_haul = categorize(distance_km)\n",
    "        return [h_basic, h_oblate, dist_karney, method, flown_km, distance_km, flight_category, flight_long_haul]    \n",
    "    \n",
    "    \n",
    "    new_cols = pd.DataFrame(df.apply(calculate_all, axis=1).tolist(),                                                   # apply main function\n",
    "                            columns=['flown_km_haversine_basic', 'flown_km_haversine_oblate', 'flown_km_karney', 'distance_method', 'flown_km', 'distance_km', 'flight_category', 'flight_long_haul'])\n",
    "    \n",
    "    for col in new_cols.columns:\n",
    "        df[col] = new_cols[col]\n",
    "        \n",
    "    return df\n",
    "\n",
    "distances(dummy, lat1_col='lat1', lon1_col='lon1', lat2_col='lat2', lon2_col='lon2', \n",
    "              flight_booked_col='flight_booked', return_flight_col='return_flight_booked', method='karney', \n",
    "              short_haul_max=1287, medium_haul_max=4023, long_haul_max=10460)\n",
    "\n",
    "display(dummy[['flight_booked', 'return_flight_booked', 'distance_method', 'flown_km', 'distance_km', 'flight_category', 'flight_long_haul']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtype(df, col_list, dtype):\n",
    "    dtype_map = {\n",
    "        'datetime': 'datetime64[ns]',\n",
    "        'float': 'float64',\n",
    "        'bool': 'bool',\n",
    "        'object': 'object',\n",
    "        'int': 'int64'}\n",
    "    target_dtype = dtype_map.get(dtype, dtype)\n",
    "    for col in col_list:\n",
    "        try:\n",
    "            df[col] = df[col].astype(target_dtype)                                      # motification to original df in arg made here\n",
    "        except ValueError:\n",
    "            if dtype == 'int':\n",
    "                df[col] = df[col].astype('float64').astype(target_dtype)                # convert str to float first then int if unable to convert str to int directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dtype_multi(df, dtype_dict):\n",
    "    dtype_map = {\n",
    "        'datetime': 'datetime64[ns]',\n",
    "        'float': 'float64',\n",
    "        'bool': 'bool',\n",
    "        'object': 'object',\n",
    "        'int': 'int64'\n",
    "    }\n",
    "    \n",
    "    for dtype, col_list in dtype_dict.items():\n",
    "        target_dtype = dtype_map.get(dtype, dtype)\n",
    "        \n",
    "        for col in col_list:\n",
    "            try:\n",
    "                df[col] = df[col].astype(target_dtype)  # In-place modification\n",
    "            except ValueError:\n",
    "                if dtype == 'int':\n",
    "                    df[col] = df[col].astype('float64').astype(target_dtype)  # Fallback for int conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_cluster(df, groupby_column='user_id', \n",
    "                 n_clusters=5, random_state=42, \n",
    "                 na_as_zero=False, numeric_only=True, n_init=10):\n",
    "    \"\"\"\n",
    "    Perform K-means clustering on a DataFrame, grouping by a specific column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): Input data.\n",
    "    - groupby_column (str): Column to group by.\n",
    "    - n_clusters (int): Number of clusters.\n",
    "    - random_state (int): Random seed for reproducibility.\n",
    "    - na_as_zero (bool): Treat NaN values as zero.\n",
    "    - numeric_only (bool): Whether to only consider numeric columns when aggregating.\n",
    "    - n_init (int): Number of time the k-means algorithm will be run with different centroid seeds.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame: DataFrame with an additional 'cluster' column indicating cluster assignments.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame to store cluster labels\n",
    "    cluster_df = pd.DataFrame(index=df[groupby_column].unique(), columns=['cluster'])\n",
    "    \n",
    "    # Handle NaN values\n",
    "    if na_as_zero:\n",
    "        df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Aggregate data by the specified column, explicitly setting numeric_only\n",
    "    agg_data = df.groupby(groupby_column).mean(numeric_only=numeric_only).fillna(0)\n",
    "    \n",
    "    # Perform K-means clustering, explicitly setting n_init\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=n_init)\n",
    "    cluster_labels = kmeans.fit_predict(agg_data)\n",
    "    \n",
    "    # Assign clusters\n",
    "    cluster_df['cluster'] = cluster_labels\n",
    "    \n",
    "    # Merge the cluster labels back to the original DataFrame\n",
    "    merged_df = pd.merge(df, cluster_df, how='left', left_on=groupby_column, right_index=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "# result_df = user_cluster(your_dataframe, groupby_column='user_id', na_as_zero=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unmapped_values(df, mapping_dict, col_to_check):                       # find deficiency in a mapping keys to the values in a column being mapped\n",
    "    all_values = set(df[col_to_check].dropna())                                 # drop NaN values\n",
    "    mapped_values = set(mapping_dict.keys())\n",
    "    unmapped_values = all_values - mapped_values\n",
    "    \n",
    "    if len(unmapped_values) > 0:\n",
    "        print(f\"Unmapped values in {col_to_check} column found:\")\n",
    "        for dest in unmapped_values:\n",
    "            print(f\"• {dest}\")\n",
    "    else:\n",
    "        print(f\"All values in {col_to_check} column are adequately mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df, head_rows=5):                                                   # summary check function\n",
    "    print(\"DATAFRAME SUMMARY\".center(80, \"=\"))\n",
    "    info_df = pd.DataFrame({\n",
    "        'Dtype': df.dtypes,\n",
    "        'Non-Null': df.notnull().sum(),\n",
    "        'Unique': df.nunique(),\n",
    "        'Duplicate': df.T.duplicated().sum(),\n",
    "        'NaN': df.isnull().sum()\n",
    "    })\n",
    "    print(info_df, f'\\nRows: {df.shape[0]}', f'Columns: {df.shape[1]}')\n",
    "    print(\"DESCRIBE\".center(80, \"=\"))\n",
    "    display(df.describe())\n",
    "    print(\"DATAFRAME HEAD\".center(80, \"-\"))\n",
    "    display(df.head(head_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy.to_csv(CONFIG_PATH[\"dummy\"], index=False)                     # export df for future use\n",
    "dummy.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\dummy_2.csv\", index=False)         # export df for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = pd.read_csv(CONFIG_PATH[\"dummy\"])                           # import df if right version is available\n",
    "dummy = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\dummy_2.csv\")               # import df if right version is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quantiles(df, var_col):\n",
    "    column_data = df[var_col]\n",
    "    print(f\"Q1 for {var_col}: {column_data.quantile(0.25)}\")\n",
    "    print(f\"Q2 for {var_col}: {column_data.quantile(0.50)}\")\n",
    "    print(f\"Q3 for {var_col}: {column_data.quantile(0.75)}\")\n",
    "# print_quantiles(dummy, 'col1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Call by assigning a new df name or original df in arg as function does not modify original df in arg: df = def(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinsert_col(df, col_list, insert_position):                                       \n",
    "    df_copy = df.copy()                                                                 # make copy of df to avoid modifying original one\n",
    "    temp_df = df_copy[col_list]                                                         # store cols to move in temp df\n",
    "    df_copy.drop(columns=col_list, inplace=True)                                        # drop selected cols in col_list\n",
    "    for idx, col in enumerate(col_list):\n",
    "        df_copy.insert(insert_position + idx, col, temp_df[col])                        # reinsert cols to desired location aka insert_position\n",
    "    return df_copy  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Call by assigning 1 new df names as fucntion does not modify original df in arg: df1 = def(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_size_matrix(df, reference_col='user_id', threshold=2):                                                                                       # sample size analysis on numeric variablesbefore and after capping at threshold multiple of stdev\n",
    "    calc_stats = lambda col, mean, std: (df[reference_col][np.abs(col - mean) <= threshold * std].nunique(), col.count())                               # exclude NaN by default\n",
    "    cols = [c for c in df.columns if c != reference_col] if reference_col else df.columns.tolist()\n",
    "    calc_stats = lambda col, mean, std: (df[reference_col][col.notna()].nunique(), df[reference_col][np.abs(col - mean) <= threshold * std].nunique())  # (stats[0][0]), stats[0][1]) for non NaN;  (stats[1][0], stats[1][1]) for non NaN & non zero\n",
    "\n",
    "    data = []\n",
    "    for col in cols:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            metrics, nz_metrics = df[col].agg(['count', 'mean', 'std']), df[df[col] != 0][col].agg(['count', 'mean', 'std'])\n",
    "            stats = [calc_stats(df[col], metrics['mean'], metrics['std']), calc_stats(df[col], nz_metrics['mean'], nz_metrics['std'])]\n",
    "            data.append([stats[0][0], stats[0][1], stats[0][1] / stats[0][0] if stats[0][0] else 0, stats[1][0], stats[1][1], stats[1][1] / stats[1][0] if stats[1][0] else 0])\n",
    "    \n",
    "    df_out = pd.DataFrame(data, columns=[f'all_non_NaN_values_for_{reference_col}', f'capped_non_NaN_values_within_{threshold}_std', f'ratio_non_NaN_values_capped', f'all_non_zero_values_for_{reference_col}', f'capped_non_zero_values_within_{threshold}_std', f'ratio_non_zero_values_capped'], index=cols).reset_index().rename(columns={\"index\": \"field\"}).reset_index(drop=True)\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Call by assigning two new df names as fucntion does not modify original df in arg: df1, df2 = def(arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, col_list=None, col_filter='use_trip_id', reference_col='session_id', \n",
    "                    methodology='stdev', threshold=2, whisker_size=0.5):\n",
    "    \"\"\"\n",
    "    Remove outliers based on either standard deviation or interquartile range.\n",
    "    \"\"\"\n",
    "    # Default to all numeric columns if col_list is None\n",
    "    col_list = col_list or df.select_dtypes(include=['number']).columns.tolist()\n",
    "    print(f\"Selected cols for {methodology}: {col_list}\")\n",
    "    \n",
    "    # Initialize an empty list to store sets of valid row indices\n",
    "    all_valid_rows = []\n",
    "    \n",
    "    for col in col_list:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            if methodology == 'stdev':\n",
    "                stats = df[df[col_filter]==True][col].agg(['mean', 'std'])\n",
    "                valid_rows = set(df[(df[col] >= stats['mean'] - threshold * stats['std']) & \n",
    "                                    (df[col] <= stats['mean'] + threshold * stats['std'])][reference_col])\n",
    "            elif methodology == 'iqr':\n",
    "                Q1, Q3 = df[df[col_filter]==True][col].quantile([0.25, 0.75])\n",
    "                valid_rows = set(df[(df[col] >= Q1 - whisker_size * (Q3 - Q1)) & \n",
    "                                    (df[col] <= Q3 + whisker_size * (Q3 - Q1))][reference_col])\n",
    "            else:\n",
    "                raise ValueError(\"Invalid methodology. Choose either 'stdev' or 'iqr'.\")\n",
    "            \n",
    "            # Union with rows where the column is NaN\n",
    "            nan_rows = set(df[df[col].isna()][reference_col])\n",
    "            all_valid_rows.append(valid_rows.union(nan_rows))\n",
    "    \n",
    "    # Intersect all sets of valid rows\n",
    "    valid_rows = set.intersection(*all_valid_rows)\n",
    "    \n",
    "    return df[df[reference_col].isin(valid_rows)], df[~df[reference_col].isin(valid_rows)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Call by assigning a dict and map to original df by assigning keys corresponding to variables chosen from original: minmax_dict = def(arg), df[key_minmax] = minmax_dict[key] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(df, col_list=None, col_filter=None, debug=True, cap=False, threshold=2): # cap=True means to reduce the min and max range to that of threshold multiple of stdev, returns series that need to be manually added into df\n",
    "    if col_list is None:\n",
    "        col_list = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    scaled_dict = {}\n",
    "    df_filtered = df[df[col_filter]] if col_filter else df                          # filter the sample based on a boolean variable if provided instead of None\n",
    "    for col in col_list:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):                              # restrict to numeric variables\n",
    "            warnings.warn(f\"The column {col} is not numeric. Skipping.\")\n",
    "            continue\n",
    "        stats = df_filtered[col].agg(['min', 'max', 'mean', 'std'])                 # sample statistics, most relevant if capping applied, else for reference only\n",
    "        min_val, max_val = stats['min'], stats['max']                               # unadulterated min and max values for variable\n",
    "        if cap:\n",
    "            min_val, max_val = max(stats['mean'] - threshold * stats['std'], min_val), min(stats['mean'] + threshold * stats['std'], max_val) # capping applied if cap=True\n",
    "        if debug:\n",
    "            print(f\"Scaling column {col}:\")\n",
    "            print(f\"- Initial: Actual Min={stats['min']:.2f}, Max={stats['max']:.2f}. Capped Min={min_val:.2f}, Max={max_val:.2f}. Mean={stats['mean']:.2f}, Std={stats['std']:.2f}\")\n",
    "        scaled_series = pd.Series([0]*len(df)) if min_val == max_val else ((df[col] - min_val) / (max_val - min_val)).clip(0, 1)\n",
    "        scaled_dict[col] = scaled_series                                            # series in the same order as the rows picked up from df, so can be manually added to the df later by assigning \n",
    "        if debug:\n",
    "            print(f\"- Scaled: Final Min={scaled_series.min():.2f}, Max={scaled_series.max():.2f}. Mean={scaled_series.mean():.2f}, Std={scaled_series.std():.2f}\\n\")\n",
    "    return scaled_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def with minmax embebded, does not modify the original df, need to call dummy_df = def(arg)\n",
    "def compute_index_scores(df=None, index_weight_dict=None, selected_index=None, col_filter=None, debug=True, cap=True, threshold=2):\n",
    "    def minmax(df, col_list=None, col_filter=None, debug=True, cap=False, threshold=2): # cap=True means to reduce the min and max range to that of threshold multiple of stdev, returns series that need to be manually added into df\n",
    "        if col_list is None:\n",
    "            col_list = df.select_dtypes(include=['number']).columns.tolist()\n",
    "        scaled_dict = {}\n",
    "        df_filtered = df[df[col_filter]] if col_filter else df                          # filter the sample based on a boolean variable if provided instead of None\n",
    "        for col in col_list:\n",
    "            if not pd.api.types.is_numeric_dtype(df[col]):                              # restrict to numeric variables\n",
    "                warnings.warn(f\"The column {col} is not numeric. Skipping.\")\n",
    "                continue\n",
    "            stats = df_filtered[col].agg(['min', 'max', 'mean', 'std'])                 # sample statistics, most relevant if capping applied, else for reference only\n",
    "            min_val, max_val = stats['min'], stats['max']                               # unadulterated min and max values for variable\n",
    "            if cap:\n",
    "                min_val, max_val = max(stats['mean'] - threshold * stats['std'], min_val), min(stats['mean'] + threshold * stats['std'], max_val) # capping applied if cap=True\n",
    "            if debug:\n",
    "                print(f\"Scaling column {col}:\")\n",
    "                print(f\"- Initial: Actual Min={stats['min']:.2f}, Max={stats['max']:.2f}. Capped Min={min_val:.2f}, Max={max_val:.2f}. Mean={stats['mean']:.2f}, Std={stats['std']:.2f}\")\n",
    "            scaled_series = pd.Series([0]*len(df)) if min_val == max_val else ((df[col] - min_val) / (max_val - min_val)).clip(0, 1)\n",
    "            scaled_dict[col] = scaled_series                                            # series in the same order as the rows picked up from df, so can be manually added to the df later by assigning \n",
    "            if debug:\n",
    "                print(f\"- Scaled: Final Min={scaled_series.min():.2f}, Max={scaled_series.max():.2f}. Mean={scaled_series.mean():.2f}, Std={scaled_series.std():.2f}\\n\")\n",
    "        return scaled_dict\n",
    "    \n",
    "    # initalizations\n",
    "    sub_key = list(index_weight_dict[selected_index].keys())[0]         # intermediate sub key needed to pull metrics from perk_dict\n",
    "    metrics = list(index_weight_dict[selected_index][sub_key].keys())   # list composition metrics to be scaled wrt index\n",
    "    print(f\"{selected_index}: {metrics}\")\n",
    "    scaled_dict = minmax(df=df, col_list=metrics, col_filter=col_filter, debug=debug, cap=cap, threshold=threshold)  # scale the metrics\n",
    "    index_scores = df.copy()                                            # initialize dummy df for index scores\n",
    "    for metric in metrics:                                              # weight scaled metrics\n",
    "        scaled_series = scaled_dict[metric]  # individual series of scaled metrics\n",
    "        weight = index_weight_dict[selected_index][sub_key][metric]  # draw relevant weight from perk_dict\n",
    "        index_scores[f\"{metric}_scaled_weighted\"] = scaled_series * weight  # weight scaled metrics\n",
    "    scaled_weighted_cols = [col for col in index_scores.columns if '_scaled_weighted' in col]  # name the columns\n",
    "    index_scores[selected_index] = index_scores[scaled_weighted_cols].sum(axis=1)  # new column for index score as sum of weighted scaled metrics\n",
    "    index_col = index_scores[selected_index]\n",
    "    scaled_df = pd.DataFrame(scaled_dict)\n",
    "    result_df = pd.DataFrame(pd.concat([scaled_df, index_col], axis=1))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data loading\n",
    "* load full data set from source (full df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Express lane: import full df (if right version available) and skip rest of the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.read_csv(CONFIG_PATH[\"full\"])                             # import full df if available\n",
    "df_name_and_variable_sizes(full, ['user_id', 'session_id'])         # print counts of unique user_id and unique session_id\n",
    "# df name is full ==> 1020926 user_id , 5408063 session_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Initial load data tables from postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep\n",
    "default_db_url = \"postgres://Test:[redacted]/TravelTide\"           # pre-filled database URL (redacted -  so it won't work if you run this)\n",
    "db_url_input = widgets.Text(                                                                                        \n",
    "    value=default_db_url,\n",
    "    placeholder='Enter your DB URL',\n",
    "    description='DB URL:',\n",
    "    disabled=False)                                                                                                 # create a textbox for the database URL input\n",
    "connect_button = widgets.Button(description=\"Connect and Execute Queries\")      # create a button to execute the connection and queries\n",
    "output = widgets.Output()                                                       # create an output widget to display messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_connect_button_clicked(b):                                               # run import process\n",
    "    local_dataframes = {}                                                       # local variable to store DataFrames\n",
    "    with output:\n",
    "        clear_output(wait=True)                                                 # clear previous messages\n",
    "        try:\n",
    "            connection = psycopg2.connect(db_url_input.value)\n",
    "            with connection.cursor() as cursor:\n",
    "                cursor.execute(\"SELECT version();\")\n",
    "                version = cursor.fetchone()\n",
    "                print(\"Connected to PostgreSQL version:\", version)\n",
    "                \n",
    "                # get all table names in the public schema\n",
    "                cursor.execute(\"\"\"\n",
    "                    SELECT table_name\n",
    "                    FROM information_schema.tables\n",
    "                    WHERE table_schema = 'public'\n",
    "                \"\"\")\n",
    "                table_names = [row[0] for row in cursor.fetchall()]\n",
    "                \n",
    "                for table_name in table_names:\n",
    "                    query = f\"SELECT * FROM {table_name};\"\n",
    "                    cursor.execute(query)\n",
    "                    data = cursor.fetchall()\n",
    "                    column_names = [desc[0] for desc in cursor.description]\n",
    "                    df = pd.DataFrame(data, columns=column_names)\n",
    "                    local_dataframes[table_name] = df\n",
    "                \n",
    "                connection.close()\n",
    "                print(\"Queries executed successfully.\")\n",
    "                \n",
    "        except psycopg2.Error as e:\n",
    "            print(\"Error connecting to the database:\", e)\n",
    "    \n",
    "    return local_dataframes                                                 # return populated dictionary\n",
    "dataframes = on_connect_button_clicked(None)                                # pass None because this function expects a button argument, but we don't need it for manual execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import tables as df and merge them to create full df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataframes.keys())                # names of loaded tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = dataframes['users']             # based on printed keys, set up dfs of the tables\n",
    "sessions = dataframes['sessions']\n",
    "flights = dataframes['flights']\n",
    "hotels = dataframes['hotels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.merge(users, sessions, on=['user_id'], how='left') # merge tables to create full df\n",
    "full = pd.merge(full, flights, on=['trip_id'], how='left')\n",
    "full = pd.merge(full, hotels, on=['trip_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full.to_csv(CONFIG_PATH[\"full\"], index=False)               # export for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Exogenous info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Destination type info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destinations = pd.DataFrame(full['destination'][~full['destination'].isna()].unique(), columns=['destination']) # initialize list of destinations from full df\n",
    "df_name_and_variable_sizes(destinations, ['destination'])                                                       # print counts of destinations\n",
    "# df name is destinations ==> 140 destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_mapping_dict = {                                                                                    # mapping dict, keys = city/destination\n",
    "    # USA\n",
    "    'los angeles': 'US', 'denver': 'US', 'dallas': 'US', 'houston': 'US', 'tucson': 'US', 'boston': 'US', 'washington': 'US', 'new york': 'US', 'chicago': 'US', \n",
    "    'jacksonville': 'US', 'detroit': 'US', 'indianapolis': 'US', 'philadelphia': 'US', 'charlotte': 'US', 'san antonio': 'US', 'miami': 'US', 'columbus': 'US', 'san jose': 'US', \n",
    "    'fort worth': 'US', 'seattle': 'US', 'phoenix': 'US', 'austin': 'US', 'portland': 'US', 'san diego': 'US', 'el paso': 'US', 'oklahoma city': 'US', 'fresno': 'US', 'louisville': 'US', \n",
    "    'milwaukee': 'US', 'nashville': 'US', 'las vegas': 'US', 'san francisco': 'US', 'baltimore': 'US', 'memphis': 'US', 'atlanta': 'US', 'orlando': 'US', 'honolulu': 'US',\n",
    "    # Canada\n",
    "    'ottawa': 'Canada', 'toronto': 'Canada', 'montreal': 'Canada', 'calgary': 'Canada', 'edmonton': 'Canada', 'winnipeg': 'Canada', 'quebec': 'Canada', 'hamilton': 'Canada', 'vancouver': 'Canada',\n",
    "    # South America\n",
    "    'rio de janeiro': 'Brazil', 'buenos aires': 'Argentina', 'lima': 'Peru', 'bogota': 'Colombia', 'quito': 'Ecuador', 'montevideo': 'Uruguay',\n",
    "    # Mexico & Central America\n",
    "    'mexico city': 'Mexico', 'havana': 'Cuba', 'punta cana': 'Dominican Republic',\n",
    "    # Europe\n",
    "    'stockholm': 'Sweden', 'copenhagen': 'Denmark', 'madrid': 'Spain', 'amsterdam': 'Netherlands', 'vienna': 'Austria', 'berlin': 'Germany', 'hamburg': 'Germany', 'london': 'UK', \n",
    "    'rome': 'Italy', 'prague': 'Czech Republic', 'paris': 'France', 'brussels': 'Belgium', 'barcelona': 'Spain', 'dublin': 'Ireland', 'warsaw': 'Poland', 'budapest': 'Hungary', \n",
    "    'moscow': 'Russia', 'lisbon': 'Portugal', 'milan': 'Italy', 'porto': 'Portugal', 'munich': 'Germany', 'geneva': 'Switzerland', 'florence': 'Italy', 'naples': 'Italy', \n",
    "    'edinburgh': 'UK', 'sofia': 'Bulgaria', 'bucharest': 'Romania', 'jerusalem': 'Israel', 'istanbul': 'Turkey', 'antalya': 'Turkey', 'heraklion': 'Greece', 'casablanca': 'Morocco', \n",
    "    'nice': 'France', 'venice': 'Italy',\n",
    "    # Middle East\n",
    "    'amman': 'Jordan', 'dubai': 'UAE', 'riyadh': 'Saudi Arabia', 'beirut': 'Lebanon', 'dammam': 'Saudi Arabia', 'abu dhabi': 'UAE',\n",
    "    # Africa\n",
    "    'cairo': 'Egypt', 'johannesburg': 'South Africa', 'durban': 'South Africa', 'cape town': 'South Africa', 'lagos': 'Nigeria', 'accra': 'Ghana', 'hurghada': 'Egypt',\n",
    "    # Asia & OCeania\n",
    "    'shanghai': 'China', 'bangalore': 'India', 'delhi': 'India', 'jaipur': 'India', 'singapore': 'Singapore', 'hong kong': 'China', 'macau': 'China', 'tokyo': 'Japan', \n",
    "    'beijing': 'China', 'jakarta': 'Indonesia', 'bangkok': 'Thailand', 'osaka': 'Japan', 'taipei': 'Taiwan', 'manila': 'Philippines', 'seoul': 'South Korea', 'kuala lumpur': 'Malaysia', \n",
    "    'hanoi': 'Vietnam', 'phuket': 'Thailand', 'ho chi minh city': 'Vietnam', 'denpasar': 'Indonesia', 'batam': 'Indonesia', 'tianjin': 'China', 'hangzhou': 'China', 'xiamen': 'China', \n",
    "    'chengdu': 'China', 'shenzhen': 'China', 'agra': 'India', 'guangzhou': 'China', 'johor bahru': 'Malaysia', 'fukuoka': 'Japan', 'dalian': 'China', \n",
    "    'guilin': 'China', \"xi'an\": 'China', 'pune': 'India', 'colombo': 'Sri Lanka', 'qingdao': 'China',\n",
    "    'melbourne': 'Australia', 'sydney': 'Australia', 'auckland': 'New Zealand'}\n",
    "find_unmapped_values(full, destination_mapping_dict, 'destination')                                             # check deficiency in destination_mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df= pd.DataFrame(list(destination_mapping_dict.items()), columns=['destination', 'destination_country'])                                            # df of mapping dict and merge into destinations df \n",
    "destinations = pd.merge(destinations, map_df, on=['destination'], how='left')                                                                           # merge mapping df into destinations df\n",
    "destinations['destination_type'] = destinations['destination_country'].apply(lambda x: 'US & Canada' if x in ['US', 'Canada'] else 'foreign travel')    # new col destination_type, US & Canada vs foreign travel\n",
    "destinations['foreign_destination'] = destinations['destination_type'] == 'foreign travel'                                                              # new col foreign_destination True/False/NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Airline type info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = pd.DataFrame(full['trip_airline'].dropna().unique(), columns=['trip_airline'])   # initialize list of airlines from full df\n",
    "df_name_and_variable_sizes(airlines, ['trip_airline'])                                      # print counts of airlines\n",
    "# df name is airlines ==> 355 trip_airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_airlines_list = [                                                                    # list of known budget airlines up to 2021\n",
    "    \"Ryanair\", \"Wizz Air\", \"Frontier Airlines\", \"Spirit Airlines\",\n",
    "    \"AirAsia\", \"AirAsia X\", \"IndiGo Airlines\", \"Jeju Air\",\n",
    "    \"Jetstar Airways\", \"Jetstar Asia Airways\",\"EasyJet\",\n",
    "    \"Cebu Pacific\", \"Pegasus Airlines\", \"Tiger Airways\", \"Tiger Airways Australia\",\n",
    "    \"Volaris\", \"Southwest Airlines\", \"Allegiant Air\", \"WestJet\", \"Norwegian Air Shuttle\",\n",
    "    \"AirTran Airways\", \"Virgin America\", \"Flybe\", \"Nok Air\", \"Spicejet\", \"Go Air\",\n",
    "    \"Lion Mentari Airlines\", \"Onur Air\", \"SunExpress\", \"Jet2.com\",\n",
    "    \"Mango\", \"Rwandair Express\", \"Air Arabia\", \"Eurowings\",\n",
    "    \"Peach\", \"Scoot\", \"WOW air\", \"Flydubai\"]                                               # Peach, Scoot, WOW air and Flydubai absent from full df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines['budget_airline'] = airlines['trip_airline'].apply(lambda x: x in budget_airlines_list)        # budget airline True/False/NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Hotel type info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = pd.DataFrame(full['hotel_name'][~full['hotel_name'].isna()].unique(), columns=['hotel_name'])                      # initialize list of hotel from full df\n",
    "hotels[['hotel_brand', 'hotel_city']] = hotels['hotel_name'].str.split(' - ', expand=True).apply(lambda x: x.str.strip())   # expand=True to split 'hotel_name' into new cols and remove leading/trailing whitespaces with str.strip() in .apply(lambda x: x.) \n",
    "df_name_and_variable_sizes(hotels, ['hotel_name'])                                                                          # print counts of hotels\n",
    "# df name is hotels ==> 2798 hotel_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dict = {                                                                                                            # hotel brand star ratings\n",
    "    'Accor': {'rating': 'Varies', 'notes': 'Ranges from budget (e.g., Ibis) to luxury (e.g., Sofitel)', 'luxury_indicator': False},\n",
    "    'Aman Resorts': {'rating': '5 star', 'notes': 'Luxury hotel chain', 'luxury_indicator': True},\n",
    "    'Banyan Tree': {'rating': '5 star', 'notes': 'Luxury resort brand', 'luxury_indicator': True},\n",
    "    'Best Western': {'rating': '2.5-4 star', 'notes': 'Varies with sub-brand like Best Western Plus', 'luxury_indicator': False},\n",
    "    'Choice Hotels': {'rating': 'Varies', 'notes': 'Ranges from budget to upscale', 'luxury_indicator': False},\n",
    "    'Conrad': {'rating': '5 star', 'notes': 'Hilton luxury brand', 'luxury_indicator': True},\n",
    "    'Crowne Plaza': {'rating': '4 star', 'notes': 'Part of InterContinental Hotels Group', 'luxury_indicator': True},\n",
    "    'Extended Stay': {'rating': '2.5-3.5 star', 'notes': 'Focuses on long-term stays', 'luxury_indicator': False},\n",
    "    'Fairmont': {'rating': '5 star', 'notes': 'Luxury brand', 'luxury_indicator': True},\n",
    "    'Four Seasons': {'rating': '5 star', 'notes': 'Renowned luxury brand', 'luxury_indicator': True},\n",
    "    'Hilton': {'rating': '4 star', 'notes': 'Main brand; other brands vary from 3-5 stars', 'luxury_indicator': True},\n",
    "    'Hyatt': {'rating': '4 star', 'notes': 'Main brand; other brands range from 3-5 stars', 'luxury_indicator': True},\n",
    "    'InterContinental': {'rating': '5 star', 'notes': 'Luxury brand', 'luxury_indicator': True},\n",
    "    'Marriott': {'rating': '4 star', 'notes': 'Main brand; has range from budget to luxury in other brands', 'luxury_indicator': True},\n",
    "    'NH Hotel': {'rating': '3-4 star', 'notes': 'General rating', 'luxury_indicator': False},\n",
    "    'Radisson': {'rating': '4 star', 'notes': 'Typical for main brand', 'luxury_indicator': True},\n",
    "    'Rosewood': {'rating': '5 star', 'notes': 'Luxury brand', 'luxury_indicator': True},\n",
    "    'Shangri-La': {'rating': '5 star', 'notes': 'Luxury brand', 'luxury_indicator': True},\n",
    "    'Starwood': {'rating': 'Varies', 'notes': 'Had brands ranging from 3-5 stars. Acquired by Marriott in 2016.', 'luxury_indicator': False},\n",
    "    'Wyndham': {'rating': '3-4 star', 'notes': 'Main brand rating; has a range from budget to upscale in other brands', 'luxury_indicator': False}}\n",
    "find_unmapped_values(hotels, ratings_dict, 'hotel_brand')                                                                   # check deficiency in ratings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df = pd.DataFrame.from_dict(ratings_dict, orient='index').reset_index()         # df of ratings_dict, orient='index' to convert dict keys to index, reset_index so index keys is new colu \n",
    "ratings_df.columns = ['hotel_brand', 'hotel_star', 'notes', 'luxury_hotel']             # assign column names, luxury_hotel is bool\n",
    "ratings_df = ratings_df.drop(columns=['notes'])                                         # drop notes column\n",
    "df_name_and_variable_sizes(hotels, ['hotel_name', 'hotel_brand'])                       # print counts of hotel names and hotel brands in ratings_dict\n",
    "# df name is hotels ==> 2798 hotel_name , 20 hotel_brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotels = hotels.merge(ratings_df, on='hotel_brand', how='left')                         # merge ratings_df to hotels df\n",
    "find_unmapped_values(hotels, destination_mapping_dict, 'hotel_city')                    # check deficiency in destination_mapping_dict to map hotel_city to country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_hotel_df= pd.DataFrame(list(destination_mapping_dict.items()), columns=['hotel_city', 'hotel_country'])                         # df of destination_mapping_dict but mapped to hotel_city\n",
    "hotels = pd.merge(hotels, map_hotel_df, on=['hotel_city'], how='left')                                                              # merge mapping df into hotels df\n",
    "hotels['hotel_location'] = hotels['hotel_country'].apply(lambda x: 'US & Canada' if x in ['US', 'Canada'] else 'foreign stay')      # new col if hotel is in US & Canada or a foreign stay\n",
    "hotels['foreign_stay'] = hotels['hotel_location'] == 'foreign stay'                                                                 # new col foreign_stay True/False/NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Cohort data\n",
    "* Cohort full df, merge exogenous info and generate transaction related metrics (data df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Express lane: Import data df (if right version available) and skip rest of the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete without care as will affect other sections\n",
    "data_dtype_dict = {\n",
    "    'datetime': ['birthdate', 'sign_up_date', 'session_start', 'session_end', \n",
    "                 'departure_time', 'return_time', 'check_in_time', 'check_out_time', \n",
    "                 'travel_start', 'travel_end', 'trip_booking_time'],\n",
    "    'float': ['base_fare_usd_per_km', 'hotel_per_room_usd','search_session_clicks', 'search_session_mins'],\n",
    "    'object': ['user_id']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CONFIG_PATH[\"data\"])                                         # import data df if available\n",
    "df_name_and_variable_sizes(data, ['user_id', 'session_id'])                     # print counts of unique user_id and unique session_id and get info\n",
    "# df name is data ==> 5998 user_id , 49211 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dtype_multi(data, dtype_dict=data_dtype_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Cohort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = full.copy()                                                  # initialize data df by creating a copy on full df, make sure full df is loaded, and if not go to previous steps above to load\n",
    "df_name_and_variable_sizes(data, ['user_id', 'session_id'])         # print counts of unique user_id and unique session_id and get info\n",
    "# df name is data ==> 1020926 user_id , 5408063 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce scope per Elena Tarrant request\n",
    "data = data[data['session_start'] >= pd.Timestamp('2023-01-04 00:00:00')]        # filter for cut-off datetime 2023-01-04 00:00:00\n",
    "data = data[data.groupby('user_id')['session_id'].transform('nunique') > 7]      # filter for users with more than 7 sessions, .transform() generates a Series (same length as df) each element is count of unique session_ids at groupby user_id level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name_and_variable_sizes(data, ['user_id', 'session_id'])                             # print counts of unique user_id and unique session_id, should be lower than previous print\n",
    "# df name is data ==> 5998 user_id , 49211 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adhoc sense check\n",
    "\n",
    "full.hotel_booked.dropna().count() # 5408063\n",
    "full.trip_id.dropna().count() # 2335845\n",
    "full.hotel_name.dropna().count() # 1962920\n",
    "full[full.hotel_booked].hotel_name.dropna().count() # 1962920\n",
    "full[full.trip_id.notna()].hotel_name.dropna().count() # 1962920\n",
    "full[full.hotel_booked].hotel_booked.count() # 2009287, more rows with hotel_booked==True vs rows with non-NaN in hotel_name\n",
    "\n",
    "data.trip_id.dropna().count() # 16702\n",
    "data.hotel_booked.dropna().count() # 16702, includes False\n",
    "data[data.hotel_booked==True].hotel_booked.count() # 14919 # number of rows for which hotel_booked is True\n",
    "data.hotel_name.dropna().count() # 14726, again indicating some hotel_name rows are underpopulated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_1.csv\", index=False)                             # intermediate export as backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Merge exogenous info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(data, destinations, on=['destination'], how='left') # merge destinations df\n",
    "data = pd.merge(data, airlines, on=['trip_airline'], how='left') # merge airlines df\n",
    "data = pd.merge(data, hotels, on=['hotel_name'], how='left') # merge hotels df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these columns are only applicable for where trip_id != NaN, outside of that it is NaN, some specific to flight_booked, hotel_booked and hotel_name\n",
    "col_tidy = [# boolean type\n",
    "            'flight_booked', 'hotel_booked', 'cancellation', 'return_flight_booked', \n",
    "            # travel times\n",
    "            'departure_time', 'return_time', 'check_in_time', 'check_out_time','rooms', \n",
    "            # trip economics \n",
    "            'flight_discount', 'hotel_discount', 'seats', 'checked_bags', 'nights', 'base_fare_usd', 'hotel_per_room_usd', \n",
    "            # airport\n",
    "            'home_airport_lat', 'home_airport_lon', 'destination_airport_lat', 'destination_airport_lon', \n",
    "            # names \n",
    "            'origin_airport', 'destination', 'destination_airport', 'trip_airline', 'hotel_name']                                                                   # NaN if trip_id==NaN\n",
    "data.loc[data['trip_id'].isna(), col_tidy] = np.nan\n",
    "data.loc[data['trip_id'].isna() | data['flight_booked'].isna(), ['foreign_destination', 'budget_airline']] = np.nan                                           # restrict True/False variables to correct rows\n",
    "data.loc[data['trip_id'].isna() | data['hotel_name'].isna() | data['hotel_booked'].isna(), ['luxury_hotel', 'foreign_stay']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_2.csv\", index=False)                             # intermediate export as backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Work around duplicated trip_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep some cols to be the right dtype for calc in this section\n",
    "trip_id_dtype_dict = {\n",
    "    'datetime': ['birthdate', 'sign_up_date', 'session_start', 'session_end', 'departure_time', 'return_time', 'check_in_time', 'check_out_time'],\n",
    "    'object': ['user_id', 'trip_id']}\n",
    "convert_dtype_multi(data, dtype_dict=trip_id_dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max counts of session_start of trip_id: {(data.groupby('trip_id')['session_start'].nunique()).max()}\") # some trip_ids have two session_starts max, one for origination, another cancellation, only most recent row eligible for metrics\n",
    "# originating leg of some cancelled trip_id are not in the session_start cut-off requested by Elena "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame\n",
    "data.sort_values(['trip_id', 'session_start'], inplace=True)\n",
    "\n",
    "# Initialize 'use_trip_id' as NaN\n",
    "data['use_trip_id'] = np.nan\n",
    "\n",
    "# Set 'use_trip_id' as True for the most recent 'session_start' for each 'trip_id'\n",
    "data.loc[data.groupby('trip_id')['session_start'].idxmax(), 'use_trip_id'] = True\n",
    "\n",
    "# Find rows with only one session and set their 'use_trip_id' as True\n",
    "single_session_mask = ~data['trip_id'].duplicated(keep=False)\n",
    "data.loc[single_session_mask, 'use_trip_id'] = True\n",
    "\n",
    "# Set 'use_trip_id' to False for other rows where 'trip_id' is not NaN\n",
    "data.loc[data['trip_id'].notna() & data['use_trip_id'].isna(), 'use_trip_id'] = False\n",
    "\n",
    "# Finally, set 'use_trip_id' to NaN where 'trip_id' is NaN\n",
    "data.loc[data['trip_id'].isna(), 'use_trip_id'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_3.csv\", index=False)                             # intermediate export as backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Local\\Temp\\ipykernel_51964\\1526391504.py:1: DtypeWarning: Columns (12,15,16,19,20,22,23,24,25,27,28,29,31,35,38,39,41,42,43,44,45,46,47,48,49,50,51,52) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_3.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep some cols to be the right dtype for calc in this section\n",
    "distances_dtype_dict = {\n",
    "    'float': ['home_airport_lat', 'home_airport_lon', 'destination_airport_lat', 'destination_airport_lon'],\n",
    "    'object': ['user_id']}\n",
    "convert_dtype_multi(data, dtype_dict=distances_dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\lib\\pretty.py:778: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  output = repr(obj)\n",
      "C:\\Users\\Master\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\formatters.py:344: FutureWarning: In a future version, object-dtype columns with all-bool values will not be included in reductions with bool_only=True. Explicitly cast to bool dtype instead.\n",
      "  return method()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>gender</th>\n",
       "      <th>married</th>\n",
       "      <th>has_children</th>\n",
       "      <th>home_country</th>\n",
       "      <th>home_city</th>\n",
       "      <th>home_airport</th>\n",
       "      <th>home_airport_lat</th>\n",
       "      <th>home_airport_lon</th>\n",
       "      <th>...</th>\n",
       "      <th>foreign_stay</th>\n",
       "      <th>use_trip_id</th>\n",
       "      <th>flown_km_haversine_basic</th>\n",
       "      <th>flown_km_haversine_oblate</th>\n",
       "      <th>flown_km_karney</th>\n",
       "      <th>distance_method</th>\n",
       "      <th>flown_km</th>\n",
       "      <th>distance_km</th>\n",
       "      <th>flight_category</th>\n",
       "      <th>flight_long_haul</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101486</td>\n",
       "      <td>1972-12-07</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>usa</td>\n",
       "      <td>tacoma</td>\n",
       "      <td>TCM</td>\n",
       "      <td>47.138</td>\n",
       "      <td>-122.476</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>965.340568</td>\n",
       "      <td>963.184958</td>\n",
       "      <td>966.850557</td>\n",
       "      <td>karney</td>\n",
       "      <td>1933.701114</td>\n",
       "      <td>966.850557</td>\n",
       "      <td>short haul</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101486</td>\n",
       "      <td>1972-12-07</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>usa</td>\n",
       "      <td>tacoma</td>\n",
       "      <td>TCM</td>\n",
       "      <td>47.138</td>\n",
       "      <td>-122.476</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101961</td>\n",
       "      <td>1980-09-14</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>usa</td>\n",
       "      <td>boston</td>\n",
       "      <td>BOS</td>\n",
       "      <td>42.364</td>\n",
       "      <td>-71.005</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>300.162123</td>\n",
       "      <td>299.491599</td>\n",
       "      <td>300.479240</td>\n",
       "      <td>karney</td>\n",
       "      <td>600.958480</td>\n",
       "      <td>300.479240</td>\n",
       "      <td>short haul</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101961</td>\n",
       "      <td>1980-09-14</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>usa</td>\n",
       "      <td>boston</td>\n",
       "      <td>BOS</td>\n",
       "      <td>42.364</td>\n",
       "      <td>-71.005</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>400.220489</td>\n",
       "      <td>399.326543</td>\n",
       "      <td>400.244156</td>\n",
       "      <td>karney</td>\n",
       "      <td>800.488312</td>\n",
       "      <td>400.244156</td>\n",
       "      <td>short haul</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101961</td>\n",
       "      <td>1980-09-14</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>usa</td>\n",
       "      <td>boston</td>\n",
       "      <td>BOS</td>\n",
       "      <td>42.364</td>\n",
       "      <td>-71.005</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4003.264513</td>\n",
       "      <td>3994.323080</td>\n",
       "      <td>4014.164906</td>\n",
       "      <td>karney</td>\n",
       "      <td>8028.329812</td>\n",
       "      <td>4014.164906</td>\n",
       "      <td>medium haul</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49206</th>\n",
       "      <td>533809</td>\n",
       "      <td>1985-11-06</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>usa</td>\n",
       "      <td>milwaukee</td>\n",
       "      <td>MKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>karney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49207</th>\n",
       "      <td>621849</td>\n",
       "      <td>1971-06-15</td>\n",
       "      <td>F</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>usa</td>\n",
       "      <td>birmingham</td>\n",
       "      <td>BHM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>karney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49208</th>\n",
       "      <td>599790</td>\n",
       "      <td>1967-07-23</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>usa</td>\n",
       "      <td>new york</td>\n",
       "      <td>JFK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>karney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49209</th>\n",
       "      <td>435596</td>\n",
       "      <td>1970-11-28</td>\n",
       "      <td>F</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>usa</td>\n",
       "      <td>houston</td>\n",
       "      <td>EFD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>karney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49210</th>\n",
       "      <td>681720</td>\n",
       "      <td>1985-01-25</td>\n",
       "      <td>M</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>usa</td>\n",
       "      <td>new york</td>\n",
       "      <td>LGA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>karney</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49211 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id   birthdate gender  married  has_children home_country  \\\n",
       "0      101486  1972-12-07      F     True          True          usa   \n",
       "1      101486  1972-12-07      F     True          True          usa   \n",
       "2      101961  1980-09-14      F     True         False          usa   \n",
       "3      101961  1980-09-14      F     True         False          usa   \n",
       "4      101961  1980-09-14      F     True         False          usa   \n",
       "...       ...         ...    ...      ...           ...          ...   \n",
       "49206  533809  1985-11-06      F     True          True          usa   \n",
       "49207  621849  1971-06-15      F     True          True          usa   \n",
       "49208  599790  1967-07-23      F    False         False          usa   \n",
       "49209  435596  1970-11-28      F    False         False          usa   \n",
       "49210  681720  1985-01-25      M    False          True          usa   \n",
       "\n",
       "        home_city home_airport  home_airport_lat  home_airport_lon  ...  \\\n",
       "0          tacoma          TCM            47.138          -122.476  ...   \n",
       "1          tacoma          TCM            47.138          -122.476  ...   \n",
       "2          boston          BOS            42.364           -71.005  ...   \n",
       "3          boston          BOS            42.364           -71.005  ...   \n",
       "4          boston          BOS            42.364           -71.005  ...   \n",
       "...           ...          ...               ...               ...  ...   \n",
       "49206   milwaukee          MKE               NaN               NaN  ...   \n",
       "49207  birmingham          BHM               NaN               NaN  ...   \n",
       "49208    new york          JFK               NaN               NaN  ...   \n",
       "49209     houston          EFD               NaN               NaN  ...   \n",
       "49210    new york          LGA               NaN               NaN  ...   \n",
       "\n",
       "      foreign_stay use_trip_id flown_km_haversine_basic  \\\n",
       "0            False        True               965.340568   \n",
       "1            False        True                      NaN   \n",
       "2            False        True               300.162123   \n",
       "3            False        True               400.220489   \n",
       "4            False        True              4003.264513   \n",
       "...            ...         ...                      ...   \n",
       "49206          NaN         NaN                      NaN   \n",
       "49207          NaN         NaN                      NaN   \n",
       "49208          NaN         NaN                      NaN   \n",
       "49209          NaN         NaN                      NaN   \n",
       "49210          NaN         NaN                      NaN   \n",
       "\n",
       "      flown_km_haversine_oblate flown_km_karney distance_method     flown_km  \\\n",
       "0                    963.184958      966.850557          karney  1933.701114   \n",
       "1                           NaN             NaN             NaN          NaN   \n",
       "2                    299.491599      300.479240          karney   600.958480   \n",
       "3                    399.326543      400.244156          karney   800.488312   \n",
       "4                   3994.323080     4014.164906          karney  8028.329812   \n",
       "...                         ...             ...             ...          ...   \n",
       "49206                       NaN             NaN          karney          NaN   \n",
       "49207                       NaN             NaN          karney          NaN   \n",
       "49208                       NaN             NaN          karney          NaN   \n",
       "49209                       NaN             NaN          karney          NaN   \n",
       "49210                       NaN             NaN          karney          NaN   \n",
       "\n",
       "       distance_km  flight_category flight_long_haul  \n",
       "0       966.850557       short haul            False  \n",
       "1              NaN              NaN              NaN  \n",
       "2       300.479240       short haul            False  \n",
       "3       400.244156       short haul            False  \n",
       "4      4014.164906      medium haul            False  \n",
       "...            ...              ...              ...  \n",
       "49206          NaN             None             None  \n",
       "49207          NaN             None             None  \n",
       "49208          NaN             None             None  \n",
       "49209          NaN             None             None  \n",
       "49210          NaN             None             None  \n",
       "\n",
       "[49211 rows x 61 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate distance cols, includes cancelled trips, use karney method for calcs for metrics\n",
    "distances(data, lat1_col='home_airport_lat', lon1_col='home_airport_lon', lat2_col='destination_airport_lat', lon2_col='destination_airport_lon',\n",
    "              flight_booked_col='flight_booked', return_flight_col='return_flight_booked', method='karney', \n",
    "              short_haul_max=1287, medium_haul_max=4023, long_haul_max=10460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance_method should only have values where distance_km col has values\n",
    "data.loc[data['distance_km'].isna() , ['distance_method']] = np.nan                     # tidy up\n",
    "data = data.drop(columns=['flown_km_haversine_basic', 'flown_km_haversine_oblate', 'flown_km_karney']) # drop useless cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_4.csv\", index=False)                               # intermediate export as backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Boolean indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Local\\Temp\\ipykernel_51964\\3486984536.py:1: DtypeWarning: Columns (12,15,16,19,20,22,23,24,25,27,28,29,31,35,38,39,41,42,43,44,45,46,47,48,49,50,51,52,53,56,57) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_4.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general and product mix indicators\n",
    "booking = (data['trip_id'].notna()) & (data['cancellation'].notna()) & (data['use_trip_id']==True)\n",
    "flight_booking = booking & (data['flight_booked']==True)\n",
    "hotel_booking = booking & (data['hotel_booked']==True)\n",
    "combo_booking = flight_booking & hotel_booking\n",
    "\n",
    "cancellation = booking & (data['cancellation']==True) \n",
    "trip = booking & (data['cancellation']==False)\n",
    "\n",
    "flight_trip = trip & flight_booking\n",
    "hotel_trip = trip & hotel_booking \n",
    "combo_trip = trip & combo_booking\n",
    "return_trip = flight_trip & (data['return_flight_booked']==True)\n",
    "\n",
    "flight_only_trip = flight_trip & (data['hotel_booked']==False)\n",
    "hotel_only_trip = hotel_trip & (data['flight_booked']==False)\n",
    "\n",
    "# behavioral indicators\n",
    "foreign_trip = trip & ((data['foreign_stay']==True) | (data['foreign_destination']==True))\n",
    "long_haul_trip = flight_trip & (data['flight_long_haul']==True)\n",
    "staycation = hotel_trip & (data['home_city'] == data['hotel_city'])\n",
    "\n",
    "budget_flight = flight_trip & (data['budget_airline']==True)\n",
    "premium_flight = flight_trip & (data['budget_airline']==False)\n",
    "luxury_stay = hotel_trip & (data['luxury_hotel']==True)\n",
    "quality_travel = luxury_stay | premium_flight\n",
    "\n",
    "# discount indicators\n",
    "discount_trip = trip & ((data['flight_discount']==True) | (data['hotel_discount']==True))\n",
    "discount_flight_trip = flight_trip & (data['flight_discount']==True)  \n",
    "discount_hotel_trip = hotel_trip & (data['hotel_discount']==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non cancellation session\n",
    "non_cancellation_session = ~(data['cancellation']==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eligible denonminators\n",
    "data['booking'] = booking                          \n",
    "data['cancelled_booking'] = cancellation\n",
    "data['trip'] = trip\n",
    "# general product behavior     \n",
    "data['trip_with_flight'] = flight_trip\n",
    "data['trip_with_hotel'] = hotel_trip\n",
    "data['combo_trip'] = combo_trip                             \n",
    "# exclusive product behavior\n",
    "data['flight_only_trip'] = flight_only_trip\n",
    "data['hotel_only_trip'] = hotel_only_trip\n",
    "# other behavior\n",
    "data['return_trip'] = return_trip\n",
    "data['foreign_trip'] = foreign_trip\n",
    "data['long_haul_trip'] = long_haul_trip\n",
    "data['staycation'] = staycation\n",
    "# propensity to spend\n",
    "data['budget_flight'] = budget_flight\n",
    "data['premium_flight'] = premium_flight\n",
    "data['luxury_stay'] = luxury_stay\n",
    "data['quality_travel'] = quality_travel\n",
    "# bargain hunting\n",
    "data['discount_trip'] = discount_trip\n",
    "data['discount_flight_trip'] = discount_flight_trip\n",
    "data['discount_hotel_trip'] = discount_hotel_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these booleans are only applicable for where trip_id != NaN, outside of that it is NaN\n",
    "data.loc[data['trip_id'].isna() , ['booking', 'cancelled_booking', 'trip',                              # eligible denonminators\n",
    "                                   'trip_with_flight', 'trip_with_hotel', 'combo_trip',                 # general product behavior \n",
    "                                   'flight_only_trip', 'hotel_only_trip',                               # exclusive product behavior\n",
    "                                   'return_trip', 'foreign_trip', 'long_haul_trip', 'staycation',       # other behavior\n",
    "                                    'budget_flight', 'premium_flight', 'luxury_stay', 'quality_travel', # propensity to spend\n",
    "                                    'discount_trip', 'discount_flight_trip', 'discount_hotel_trip'      # bargain hunting\n",
    "                                    ]] = np.nan                                                         # tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_5.csv\", index=False)                             # intermediate export as backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Periods and durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Local\\Temp\\ipykernel_58676\\1410142805.py:1: DtypeWarning: Columns (12,15,16,19,20,22,23,24,25,27,28,29,31,35,38,39,41,42,43,44,45,46,47,48,49,50,51,52,53,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_5.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep some cols to be the right dtype for calc in this section\n",
    "periods_dtype_dict = {\n",
    "    'datetime': ['birthdate', 'sign_up_date', 'session_start', 'session_end', 'departure_time', 'return_time', 'check_in_time', 'check_out_time'],\n",
    "    'float': ['hotel_per_room_usd'],\n",
    "    'bool': ['married'],\n",
    "    'object': ['user_id']}\n",
    "convert_dtype_multi(data, dtype_dict=periods_dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['session_duration_mins'] = (data['session_end'] - data['session_start']).dt.total_seconds() / 60  # copnvert to mins float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['search_session_mins'] = 0\n",
    "data.loc[non_cancellation_session, 'search_session_mins'] = data['session_duration_mins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['search_session_clicks'] = 0\n",
    "data.loc[non_cancellation_session, 'search_session_clicks'] = data['page_clicks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monetizing bookings only\n",
    "data.loc[trip, 'travel_start'] = data[['departure_time', 'check_in_time']].min(axis=1).where((data['departure_time'].notna() & data['check_in_time'].notna()), other=data['departure_time'].combine_first(data['check_in_time']))\n",
    "data.loc[trip, 'travel_end'] = data[['return_time', 'check_out_time']].max(axis=1).where((data['return_time'].notna() & data['check_out_time'].notna()), other=data['return_time'].combine_first(data['check_out_time']))\n",
    "data.loc[trip, 'travel_period'] = data['travel_start'].dt.to_period('M')\n",
    "data.loc[trip, 'travel_days'] = np.ceil((data['travel_end'] - data['travel_start']).dt.total_seconds() / (24 * 60 * 60))                           # convert days float, duration of travel, small num of trips 1-way flights as travel_end is NaN\n",
    "\n",
    "data.loc[trip, 'trip_booking_time'] = pd.to_datetime(data['session_end'])                                                                           # time of booking non-cancelled trips\n",
    "data.loc[trip, 'days_pre_travel'] = np.ceil((data.loc[trip, 'travel_start'] - data.loc[trip, 'trip_booking_time']).dt.total_seconds() / (24 * 60 * 60))  # division to convert to days float, non-cancelled\n",
    "\n",
    "data.loc[flight_trip, 'flight_period'] = data['departure_time'].dt.to_period('M')                      # booked flight month-year periods\n",
    "data.loc[return_trip, 'days_between_flights'] = np.ceil((data['return_time'] - data['departure_time']).dt.total_seconds() / (24 * 60 * 60))    # convert days float, return_time NaN if no return flight booked\n",
    "\n",
    "data.loc[hotel_trip,'hotel_period'] = data['check_in_time'].dt.to_period('M')                          # booked hotel month-year periods\n",
    "\n",
    "data.loc[hotel_trip,'long_stay'] = data['nights'] > 7\n",
    "data.loc[trip, 'long_travel'] = data['travel_days'] > 7\n",
    "\n",
    "data.loc[trip, 'early_booking'] = data['days_pre_travel'] >= 180\n",
    "data.loc[trip, 'late_booking'] = data['days_pre_travel'] <= 7\n",
    "data.loc[trip, 'booking_60_to_180_days'] = (data['days_pre_travel'] >= 60) & (data['days_pre_travel'] < 180) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these booleans are only applicable for where trip_id != NaN, outside of that it is NaN\n",
    "data.loc[data['trip_id'].isna() , ['travel_start', 'travel_end', 'travel_period', 'travel_days',\n",
    "                                       'trip_booking_time', 'days_pre_travel',\n",
    "                                       'flight_period', 'days_between_flights',\n",
    "                                       'hotel_period', 'long_stay', 'long_travel', \n",
    "                                       'early_booking', 'late_booking', 'booking_60_to_180_days'\n",
    "                                    ]] = np.nan                                                         # tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_6.csv\", index=False)                             # intermediate export as backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Economics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Local\\Temp\\ipykernel_58676\\1542130171.py:1: DtypeWarning: Columns (12,15,16,19,20,22,23,24,25,27,28,29,31,35,38,39,41,42,43,44,45,46,47,48,49,50,51,52,53,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,80,81,82,84,86,88,89,90,91,92,93) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_6.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep some cols to be the right dtype for calc in this section\n",
    "economics_dtype_dict = {\n",
    "    'datetime': ['birthdate', 'sign_up_date', 'session_start', 'session_end', \n",
    "                 'departure_time', 'return_time', 'check_in_time', 'check_out_time', \n",
    "                 'travel_start', 'travel_end', 'trip_booking_time'],\n",
    "    'float': ['flight_discount_amount', 'hotel_discount_amount', 'hotel_per_room_usd', 'distance_km', 'base_fare_usd', 'seats', 'rooms', 'nights', 'checked_bags'],\n",
    "    'bool': ['married'],\n",
    "    'object': ['user_id']}\n",
    "convert_dtype_multi(data, dtype_dict=economics_dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these booleans are only applicable for where trip_id != NaN, outside of that it is NaN\n",
    "data.loc[data['trip_id'].isna() , ['booking', 'cancelled_booking', 'trip',                              # eligible denonminators\n",
    "                                   'trip_with_flight', 'trip_with_hotel', 'combo_trip',                 # general product behavior \n",
    "                                   'flight_only_trip', 'hotel_only_trip',                               # exclusive product behavior\n",
    "                                   'return_trip', 'foreign_trip', 'long_haul_trip', 'staycation',       # other behavior\n",
    "                                    'budget_flight', 'premium_flight', 'luxury_stay', 'quality_travel', # propensity to spend\n",
    "                                    'discount_trip', 'discount_flight_trip', 'discount_hotel_trip',     # bargain hunting\n",
    "                                    'early_booking', 'late_booking', 'booking_60_to_180_days'           # pre-booking     \n",
    "                                    ]] = np.nan                                                         # tidy up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate travellers\n",
    "data['num_travellers'] = data.apply(\n",
    "    lambda row: \n",
    "        max(row['rooms'], row['seats']) if (row['trip'] == True) and pd.notna(row['rooms']) and pd.notna(row['seats'])         # higher of two, which likely to be seat\n",
    "        else row['rooms'] if (row['trip'] == True) and pd.notna(row['rooms'])                                                  # more accurate since one person per seat\n",
    "        else row['seats'] if (row['trip'] == True) and pd.notna(row['seats'])                                                 # relying on this alone may underestimate travellers due to couples or groups traveling\n",
    "        else np.nan, \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rates and intensities\n",
    "data['bags_per_seat'] = np.where(                                                    \n",
    "    (data['seats'] != 0) & (data['seats'].notna()),\n",
    "    data['checked_bags'] / data['seats'],\n",
    "    np.nan)\n",
    "\n",
    "data['guests_per_room'] = np.where(                                                    \n",
    "    (data['rooms'] != 0) & (data['rooms'].notna()),\n",
    "    data['num_travellers'] / data['rooms'],\n",
    "    np.nan)\n",
    "\n",
    "data['base_fare_usd_per_km'] = np.where(                                                    \n",
    "    (data['distance_km'] != 0) & (data['distance_km'].notna()),\n",
    "    data['base_fare_usd'] / data['distance_km'],\n",
    "    np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gross value of flight booking - net of cancellation\n",
    "data['flight_gross_value_usd'] = np.where(                                                    \n",
    "    flight_trip & data['base_fare_usd'].notna() & data['seats'].notna(), \n",
    "    data['base_fare_usd'] * data['seats'],\n",
    "    0)\n",
    "\n",
    "# flight discount amount in usd\n",
    "data['flight_discount_usd'] = np.where(                                                    \n",
    "    discount_flight_trip & data['flight_discount_amount'].notna(),\n",
    "    data['flight_gross_value_usd'] * data['flight_discount_amount'], \n",
    "    0)\n",
    "\n",
    "data['flight_actual_cost_usd'] = data['flight_gross_value_usd'] - data['flight_discount_usd']   # actual flight cost in usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# room nights booked - net of cancellation\n",
    "data['room_nights'] = np.where(\n",
    "    hotel_trip &  (data['rooms']>=0) & (data['nights']>=0),\n",
    "    data['rooms'] * data['nights'],\n",
    "    0)\n",
    "\n",
    "# gross value of hotel booking - net of cancellation\n",
    "data['hotel_gross_value_usd'] = np.where(                                                    \n",
    "    hotel_trip & data['hotel_per_room_usd'].notna(), \n",
    "    data['room_nights'] * data['hotel_per_room_usd'],\n",
    "    0)\n",
    "\n",
    "# hotel discount amount in usd\n",
    "data['hotel_discount_usd'] = np.where(                                                    \n",
    "    discount_hotel_trip & data['hotel_discount_amount'].notna(),\n",
    "    data['hotel_gross_value_usd'] * data['hotel_discount_amount'], \n",
    "    0)\n",
    "\n",
    "data['hotel_actual_cost_usd'] = data['hotel_gross_value_usd'] - data['hotel_discount_usd']  # actual hotel cost in usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['trip_gross_value_usd'] = data['flight_gross_value_usd'] + data['hotel_gross_value_usd']       # gross value of trip - net of cancellation\n",
    "data['trip_discount_usd'] = data['flight_discount_usd'] + data['hotel_discount_usd']                # trip discount amount in usd\n",
    "data['trip_actual_cost_usd'] = data['flight_actual_cost_usd'] + data['hotel_actual_cost_usd']       # actual trip cost in usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['value_per_traveller'] = data['trip_actual_cost_usd'] / data['num_travellers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['value_per_seat_per_km'] = (data['flight_actual_cost_usd'] / data['seats'] ) / data['distance_km']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['travel_location'] = np.where(data['hotel_city'].notna(), data['hotel_city'], \n",
    "    np.where(data['destination'].notna(), data['destination'], \n",
    "        np.nan))                                                                                    # equivalent to multi-blanket if statement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time i.e. travel period and locations matter in determining costs of flight, hotel\n",
    "# below looks at relative discount ratio of cost of flight, hotel or overall trip to that average for same time and location\n",
    "# at overall trip level, adjustment is not made for single product bookings, though it is assumed single product bookings reflect greater frugality\n",
    "# goal is to estimate frugality / bargain hunting behavior, controlled for time and location\n",
    "\n",
    "# for flight\n",
    "data.loc[flight_trip, 'comparable_flight_discount_ratio'] = 1 - (data['flight_actual_cost_usd'].div(\n",
    "    data.groupby(['flight_period', 'destination'])['flight_actual_cost_usd']\n",
    "    .transform('mean')))\n",
    "\n",
    "# equivalent below for hotel cost\n",
    "data.loc[hotel_trip, 'comparable_hotel_discount_ratio'] = 1 - (data['hotel_actual_cost_usd'].div(\n",
    "    data.groupby(['hotel_period', 'hotel_city'])['hotel_actual_cost_usd']\n",
    "    .transform('mean')))\n",
    "\n",
    "# equivalent below for overall trip cost\n",
    "data.loc[trip, 'comparable_trip_discount_ratio'] = 1 - (data['trip_actual_cost_usd'].div(\n",
    "    data.groupby(['travel_period', 'travel_location'])['trip_actual_cost_usd']\n",
    "    .transform('mean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['days_cut_off_vs_sign_up'] = np.ceil((data['session_end'].max() - data['sign_up_date']).dt.total_seconds() / (24 * 60 * 60)) # convert days float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_7.csv\", index=False)                             # intermediate export as backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Finalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Local\\Temp\\ipykernel_58676\\2715002540.py:1: DtypeWarning: Columns (12,15,16,19,20,22,23,24,25,27,28,29,31,35,38,39,41,42,43,44,45,46,47,48,49,50,51,52,53,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,80,81,82,84,86,88,89,90,91,92,93,110) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_7.csv\")\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\\\data\\\\tt\\\\py\\\\data_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(list(data.columns), columns = ['var']).to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\columns.csv\", index=False)  # check cols manually "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_var = [\n",
    "    'user_id', 'birthdate', 'home_city', 'sign_up_date', 'days_cut_off_vs_sign_up', # user personal details\n",
    "    \n",
    "    'session_id', 'page_clicks', 'search_session_clicks', 'session_start', 'session_end', 'session_duration_mins', 'search_session_mins', # session metrics\n",
    "\n",
    "    'trip_id', 'use_trip_id', 'booking', 'cancelled_booking',                                                       # general bookings and cancellations\n",
    "\n",
    "    'trip', 'combo_trip', 'discount_trip', 'foreign_trip', 'quality_travel', 'travel_location',                     # monetizing bookings - general\n",
    "    'travel_start', 'travel_end', 'travel_period', 'travel_days', 'long_travel',                                    # monetizing bookings - travel duration\n",
    "    'trip_booking_time', 'days_pre_travel', 'early_booking', 'late_booking', 'booking_60_to_180_days',              # monetizing bookings - prebooking tendency\n",
    "    \n",
    "    'flight_period', 'trip_with_flight', 'flight_only_trip', 'discount_flight_trip', 'flight_category', 'long_haul_trip', 'return_trip', 'budget_flight', 'premium_flight', # flights\n",
    "    'trip_airline', 'flight_discount_amount', 'departure_time', 'return_time',                                                      # flights\n",
    "    'home_airport_lat', 'home_airport_lon', 'destination_airport_lat', 'destination_airport_lon', 'destination',                    # flights\n",
    "\n",
    "    'hotel_period', 'trip_with_hotel', 'hotel_only_trip', 'discount_hotel_trip', 'long_stay', 'staycation', 'luxury_stay',      # hotels\n",
    "    'hotel_brand', 'hotel_discount_amount',  'check_in_time', 'check_out_time', 'hotel_city',                                   # hotels\n",
    "    \n",
    "    'flight_gross_value_usd', 'flight_discount_usd', 'flight_actual_cost_usd',                                      # economics\n",
    "    'hotel_gross_value_usd', 'hotel_discount_usd', 'hotel_actual_cost_usd',                                         # economics\n",
    "    'trip_gross_value_usd', 'trip_discount_usd', 'trip_actual_cost_usd',                                            # economics\n",
    "    \n",
    "    'num_travellers',                                                                                               # product and economic drivers\n",
    "    'guests_per_room', 'nights', 'rooms', 'room_nights',                                                            # product and economic drivers\n",
    "    'base_fare_usd', 'seats', 'bags_per_seat', 'checked_bags', 'distance_km',                                       # product and economic drivers\n",
    "    \n",
    "    'base_fare_usd_per_km', 'hotel_per_room_usd',                                                                   # economic intensities\n",
    "\n",
    "    'value_per_traveller', 'value_per_seat_per_km',                                                                 # customer value intensities\n",
    "    'comparable_flight_discount_ratio', 'comparable_hotel_discount_ratio', 'comparable_trip_discount_ratio'         # discount intensities\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dtype_multi(data, dtype_dict=data_dtype_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export for future ise\n",
    "data.to_csv(CONFIG_PATH[\"data\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E. Clean and remove outliers\n",
    "* Create a clean version of data df (clean df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Local\\Temp\\ipykernel_58676\\2842690260.py:2: DtypeWarning: Columns (12,13,14,15,16,17,18,19,20,21,22,23,24,26,27,29,30,31,32,33,34,35,36,37,38,39,40,41,43,44,49,50,51,52,53,54,55,56,57,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(CONFIG_PATH[\"data\"]) # import data df if available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df name is data ==> 5998 user_id , 49211 session_id\n"
     ]
    }
   ],
   "source": [
    "# prep\n",
    "data = pd.read_csv(CONFIG_PATH[\"data\"]) # import data df if available\n",
    "convert_dtype_multi(data, dtype_dict=data_dtype_dict)\n",
    "df_name_and_variable_sizes(data, ['user_id', 'session_id'])            # print counts of unique user_id and unique session_id\n",
    "# df name is data ==> 5998 user_id , 49211 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df name is precut ==> 5998 user_id , 49104 session_id\n"
     ]
    }
   ],
   "source": [
    "precut = data.loc[~(data['nights'] < 0)].copy()    # .loc to make sure we work with rows where 'nights' are non-negative\n",
    "df_name_and_variable_sizes(precut, ['user_id', 'session_id'])       # print counts of unique user_id and unique session_id\n",
    "# df name is precut ==> 5998 user_id , 49104 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "economic_intensivity = ['hotel_per_room_usd', 'value_per_seat_per_km']            # intensitivy of trip economics to test outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected cols for stdev: ['hotel_per_room_usd', 'value_per_seat_per_km']\n",
      "df name is data_2_stdev ==> 5998 user_id , 47980 session_id\n"
     ]
    }
   ],
   "source": [
    "data_2_stdev, data_ex_2_stdev = remove_outliers(precut, col_list=economic_intensivity, col_filter='use_trip_id', reference_col='session_id', \n",
    "                    methodology='stdev', threshold=2, whisker_size=0.5) # stdev method, no impact on calculation from IQR whisker \n",
    "\n",
    "df_name_and_variable_sizes(data_2_stdev, ['user_id', 'session_id'])    # print counts of unique user_id and unique session_id\n",
    "# df name is data_2_stdev ==> 5998 user_id , 47980 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected cols for iqr: ['hotel_per_room_usd', 'value_per_seat_per_km']\n",
      "df name is data_IQR_whisker ==> 5998 user_id , 42822 session_id\n"
     ]
    }
   ],
   "source": [
    "data_IQR_whisker, data_ex_IQR_whisker = remove_outliers(precut, col_list=economic_intensivity, col_filter='use_trip_id', reference_col='session_id', \n",
    "                    methodology='iqr', threshold=2, whisker_size=0.5) # iqr method, no impact on calculation from stdev threshold\n",
    "\n",
    "df_name_and_variable_sizes(data_IQR_whisker, ['user_id', 'session_id'])    # print counts of unique user_id and unique session_id\n",
    "# df name is data_IQR_whisker ==> 5998 user_id , 42822 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = data_2_stdev.copy() # assign to the choice of data sets above\n",
    "outliers = data_ex_2_stdev.copy() # assign to the choice of data sets above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean.to_csv(CONFIG_PATH[\"clean\"], index=False)       # export for future use\n",
    "outliers.to_csv(CONFIG_PATH[\"outliers\"], index=False)       # export for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. User behavioral metrics    \n",
    "* Create user metrics (user df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Express lane: Import clean df (if right version available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Local\\Temp\\ipykernel_58676\\3023440181.py:1: DtypeWarning: Columns (12,13,14,15,16,17,18,19,20,21,22,23,24,26,27,29,30,31,32,33,34,35,36,37,38,39,40,41,43,44,49,50,51,52,53,54,55,56,57,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  clean = pd.read_csv(CONFIG_PATH[\"clean\"])                        # import data df if available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df name is clean ==> 5998 user_id , 47980 session_id\n"
     ]
    }
   ],
   "source": [
    "clean = pd.read_csv(CONFIG_PATH[\"clean\"])                        # import data df if available\n",
    "clean_dtype_dict = data_dtype_dict\n",
    "convert_dtype_multi(clean, dtype_dict=clean_dtype_dict)          # clean and data df have same columns hence same columns need prepping for correct dtype\n",
    "df_name_and_variable_sizes(clean, ['user_id', 'session_id'])     # print counts of unique user_id and unique session_id\n",
    "# df name is clean ==> 5998 user_id , 47677 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_var = [ \n",
    "    'user_id', 'home_city', 'sign_up_date', 'days_cut_off_vs_sign_up', # user personal details\n",
    "    \n",
    "    'session_id', 'page_clicks', 'search_session_clicks', 'session_duration_mins', 'search_session_mins',   # session metrics\n",
    "\n",
    "    'trip_id', 'use_trip_id', 'booking', 'cancelled_booking',                                               # general bookings and cancellations\n",
    "\n",
    "    'travel_location', 'travel_period', 'travel_days',                                                      # monetizing bookings - general\n",
    "    'trip', 'combo_trip', 'discount_trip', 'foreign_trip', 'quality_travel', 'long_travel',                 # monetizing bookings - general\n",
    "    \n",
    "    'trip_booking_time', 'days_pre_travel', 'early_booking', 'late_booking', 'booking_60_to_180_days',      # monetizing bookings - prebooking tendency\n",
    "        \n",
    "    'destination',  'flight_period',                                            # flights\n",
    "    'trip_with_flight', 'flight_only_trip', 'discount_flight_trip',             # flights\n",
    "    'long_haul_trip', 'return_trip', 'budget_flight', 'premium_flight',         # flights\n",
    "    \n",
    "    'hotel_city',   'hotel_period',                                             # hotels\n",
    "    'trip_with_hotel', 'hotel_only_trip', 'discount_hotel_trip',                # hotels\n",
    "    'long_stay', 'staycation', 'luxury_stay',                                   # hotels\n",
    "    \n",
    "    'flight_discount_amount', 'hotel_discount_amount',    \n",
    "    'flight_gross_value_usd', 'flight_discount_usd', 'flight_actual_cost_usd',                                      # economics\n",
    "    'hotel_gross_value_usd', 'hotel_discount_usd', 'hotel_actual_cost_usd',                                         # economics\n",
    "    'trip_gross_value_usd', 'trip_discount_usd', 'trip_actual_cost_usd',                                            # economics\n",
    "\n",
    "    'num_travellers',                                                                                               # product and economic drivers\n",
    "    'guests_per_room', 'nights', 'rooms', 'room_nights',                                                            # product and economic drivers\n",
    "    'base_fare_usd', 'seats', 'bags_per_seat', 'checked_bags', 'distance_km',                                       # product and economic drivers\n",
    "\n",
    "    'base_fare_usd_per_km', 'hotel_per_room_usd',                                                                   # economic intensities\n",
    "\n",
    "    'value_per_traveller', 'value_per_seat_per_km',                                                                 # customer value intensities\n",
    "    'comparable_flight_discount_ratio', 'comparable_hotel_discount_ratio', 'comparable_trip_discount_ratio'         # discount intensities\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No repeated strings.\n"
     ]
    }
   ],
   "source": [
    "# Check for repeats\n",
    "if len(essential_var) != len(set(essential_var)):\n",
    "    print(\"There are repeated strings.\")\n",
    "\n",
    "    # Identify repeated strings\n",
    "    from collections import Counter\n",
    "    counts = Counter(essential_var)\n",
    "    repeated = [item for item, count in counts.items() if count > 1]\n",
    "    print(f\"Repeated strings are: {repeated}\")\n",
    "else:\n",
    "    print(\"No repeated strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. User metrics (user df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = (clean[essential_var]).groupby('user_id')   # group df\n",
    "user = pd.DataFrame()                               # initialize empty df for user metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denominators - transaction counts\n",
    "user['sessions'] = group['session_id'].nunique()\n",
    "user['searches'] = group.apply(lambda x: x[(x['search_session_clicks'].notna()) & (x['search_session_mins'].notna())]['session_id'].nunique())\n",
    "user['bookings'] = group.apply(lambda x: x[x['booking']==True]['trip_id'].nunique())\n",
    "user['non_booking_sessions'] = user['searches'] - user['bookings']\n",
    "user['cancellations'] = group.apply(lambda x: x[x['cancelled_booking']==True]['trip_id'].nunique())                   \n",
    "user['trips'] = group.apply(lambda x: x[x['trip']==True]['trip_id'].nunique())\n",
    "user['flight_trip'] = group.apply(lambda x: x[x['trip_with_flight']==True]['trip_id'].nunique())               \n",
    "user['hotel_trip'] = group.apply(lambda x: x[x['trip_with_hotel']==True]['trip_id'].nunique())\n",
    "user['flight_only_trip'] = group.apply(lambda x: x[x['flight_only_trip']==True]['trip_id'].nunique())          \n",
    "user['hotel_only_trip'] = group.apply(lambda x: x[x['hotel_only_trip']==True]['trip_id'].nunique())\n",
    "# product volumes\n",
    "user['travellers'] = group.apply(lambda x: x[x['trip']==True]['num_travellers'].sum())\n",
    "user['seats'] = group.apply(lambda x: x[x['trip_with_flight']==True]['seats'].sum())\n",
    "user['checked_bags'] = group.apply(lambda x: x[x['trip_with_flight']==True]['checked_bags'].sum())\n",
    "user['hotel_guests'] = group.apply(lambda x: x[x['trip_with_hotel']==True]['num_travellers'].sum())\n",
    "user['nights'] = group.apply(lambda x: x[x['trip_with_hotel']==True]['nights'].sum())\n",
    "user['rooms'] = group.apply(lambda x: x[x['trip_with_hotel']==True]['nights'].sum())\n",
    "user['room_units'] = group.apply(lambda x: x[x['trip_with_hotel']==True]['room_nights'].sum())\n",
    "# value - usd\n",
    "user['gmv'] = group.apply(lambda x: x[x['trip']==True]['trip_gross_value_usd'].sum())           # total customer gross value pre  discount\n",
    "user['savings'] = group.apply(lambda x: x[x['trip']==True]['trip_discount_usd'].sum())          # total customer savings from discount\n",
    "user['tcv'] = group.apply(lambda x: x[x['trip']==True]['trip_actual_cost_usd'].sum())           # total customer value \n",
    "user['flight_tcv'] = group.apply(lambda x: x[x['trip']==True]['flight_actual_cost_usd'].sum())  # total customer value \n",
    "user['hotel_tcv'] = group.apply(lambda x: x[x['trip']==True]['hotel_actual_cost_usd'].sum())    # total customer value \n",
    "# seasoning\n",
    "user['days_cut_off_vs_sign_up'] = group['days_cut_off_vs_sign_up'].mean()\n",
    "user['avg_days_pre_travel'] = group['days_pre_travel'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop at the end\n",
    "# general trips\n",
    "user['combo_trip'] = group.apply(lambda x: x[x['combo_trip']==True]['trip_id'].nunique())                      \n",
    "user['foreign_trip'] = group.apply(lambda x: x[x['foreign_trip']==True]['trip_id'].nunique())                                            \n",
    "user['discount_trip'] = group.apply(lambda x: x[x['discount_trip']==True]['trip_id'].nunique())                \n",
    "user['quality_travel'] = group.apply(lambda x: x[x['quality_travel']==True]['trip_id'].nunique())              \n",
    "\n",
    "# drop at the end\n",
    "# flight trips\n",
    "user['long_haul_trip'] = group.apply(lambda x: x[x['long_haul_trip']==True]['trip_id'].nunique())              \n",
    "user['budget_flight'] = group.apply(lambda x: x[x['budget_flight']==True]['trip_id'].nunique())                \n",
    "user['premium_flight'] = group.apply(lambda x: x[x['premium_flight']==True]['trip_id'].nunique())              \n",
    "user['extended_return_trip'] = group.apply(lambda x: x[(x['return_trip']==True) & (x['long_travel']==True)]['trip_id'].nunique())\n",
    "\n",
    "# drop at the end\n",
    "# hotel trips\n",
    "user['staycation'] = group.apply(lambda x: x[x['staycation']==True]['trip_id'].nunique())                            \n",
    "user['long_stay'] = group.apply(lambda x: x[x['long_stay']==True]['trip_id'].nunique())  \n",
    "\n",
    "# drop at the end\n",
    "# pre-booking tendency\n",
    "user['early_booking'] = group.apply(lambda x: x[x['early_booking']==True]['trip_id'].nunique())  \n",
    "user['late_booking'] = group.apply(lambda x: x[x['late_booking']==True]['trip_id'].nunique())\n",
    "user['booking_60_to_180_days'] = group.apply(lambda x: x[x['booking_60_to_180_days']==True]['trip_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "user['monetization_rate'] = user['trips'].div(user['searches'])                                     # preferred perks (free hotel meal, free checked bag, one night free hotel with flight)\n",
    "# user['trips']                                                                                     # preferred perks \n",
    "# user['days_cut_off_vs_sign_up']                                                                   # preferred perks\n",
    "user['travellers_per_trip'] = group.apply(lambda x: x[x['trip']==True]['num_travellers'].mean())    # preferred perks\n",
    "user['tcv_per_traveller'] = user['tcv'].div(user['travellers'])                                     # preferred perks\n",
    "user['tcv_per_room_unit'] = user['tcv'].div(user['room_units'])                                     # preferred perks\n",
    "user['avg_tcv_per_seat_per_km'] = group['value_per_seat_per_km'].mean()                             # preferred perks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "user['quality_travel_ratio'] = user['quality_travel'].div(user['trips'])                # free hotel meal\n",
    "user['hotel_guests_per_trip'] = user['hotel_guests'].div(user['hotel_trip'])            # free hotel meal\n",
    "user['long_stay_ratio'] = user['long_stay'].div(user['hotel_trip'])                     # free hotel meal\n",
    "user['staycation_ratio'] = user['staycation'].div(user['hotel_trip'])                   # free hotel meal\n",
    "user['foreign_trip_ratio'] = user['foreign_trip'].div(user['trips'])                    # free hotel meal\n",
    "user['long_haul_ratio'] = user['long_haul_trip'].div(user['flight_trip'])               # free hotel meal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "user['premium_flight_ratio'] = user['premium_flight'].div(user['flight_trip'])               # one night free hotel with flight\n",
    "user['flight_only_ratio'] = user['flight_only_trip'].div(user['trips'])                      # one night free hotel with flight\n",
    "user['seats_per_trip'] = user['seats'].div(user['flight_trip'])                              # one night free hotel with flight\n",
    "user['extended_return_trip_ratio'] = user['extended_return_trip'].div(user['flight_trip'])   # one night free hotel with flight\n",
    "# user['foreign_trip_ratio']                                                                 # one night free hotel with flight\n",
    "# user['long_haul_ratio']                                                                    # one night free hotel with flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user['seats_per_trip']                                                                # free checked bag\n",
    "user['max_bags_per_seat'] = group.apply(lambda x: x[x['trip_with_flight']==True]['bags_per_seat'].max()) # free checked bag   \n",
    "user['budget_flight_ratio'] = user['budget_flight'].div(user['flight_trip'])            # free checked bag\n",
    "# user['extended_return_trip_ratio']                                                    # free checked bag\n",
    "# user['foreign_trip_ratio']                                                            # free checked bag\n",
    "# user['long_haul_ratio']                                                               # free checked bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user[cancellations]                                                               # no cancellation fees\n",
    "user['cancel_rate'] = user['cancellations'].div(user['bookings'])                   # no cancellation fees\n",
    "user['conversion_rate'] = user['bookings'].div(user['searches'])                    # no cancellation fees \n",
    "user['late_booking_ratio'] = user['late_booking'].div(user['trips'])                # no cancellation fees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "user['slippage_rate'] = user['non_booking_sessions'].div(user['searches'])                                                  # exclusive discounts\n",
    "user['avg_comparable_trip_discount'] = group.apply(lambda x: x[x['trip']==True]['comparable_trip_discount_ratio'].mean())   # exclusive discounts\n",
    "user['discount_trip_ratio'] = user['discount_trip'].div(user['trips'])                                                      # exclusive discounts\n",
    "user['clicks_per_session'] = (group['search_session_clicks'].sum()).div(user['searches'])                                   # exclusive discounts\n",
    "user['browsing_mins_per_session'] = (group['search_session_mins'].sum()).div(user['searches'])                              # exclusive discounts\n",
    "# user['avg_days_pre_travel']                                                                                               # exclusive discounts\n",
    "# user['late_booking_ratio']                                                                                                # exclusive discounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "user['early_booking_ratio'] = user['early_booking'].div(user['trips'])                                    # backup for exclusive discounts                  \n",
    "user['60_to_180_day_advance_booking_ratio'] = user['booking_60_to_180_days'].div(user['trips'])           # backup for exclusive discounts                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.reset_index(inplace=True) # reset the index to make user_id a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_col_drop = [\n",
    "    'combo_trip',\n",
    "    'foreign_trip',\n",
    "    'discount_trip',\n",
    "    'quality_travel',\n",
    "    'long_haul_trip',\n",
    "    'budget_flight',\n",
    "    'premium_flight',\n",
    "    'extended_return_trip',\n",
    "    'staycation',\n",
    "    'long_stay',\n",
    "    'early_booking',\n",
    "    'late_booking',\n",
    "    'booking_60_to_180_days'\n",
    "]\n",
    "user = user.drop(columns=user_col_drop)         # remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tidy up\n",
    "convert_dtype(user, list(user.columns), dtype='float')\n",
    "convert_dtype(user, ['user_id'], dtype='object')\n",
    "user.fillna(0, inplace=True)                        # replace NaN with 0\n",
    "user.replace([np.inf, -np.inf], 0, inplace=True)    # replace inf and -inf with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.to_csv(CONFIG_PATH[\"user\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Correlation and sample size analyses => corr_matrix df & sample_size_matrix df\n",
    "* Pearson correlation: Strong (|r| ≥ 0.7), Moderate (0.3≤ ∣r∣ <0.70), Weak (∣r∣ < 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df name is user ==> 5998 user_id\n"
     ]
    }
   ],
   "source": [
    "user = pd.read_csv(CONFIG_PATH[\"user\"])                        # import data df if available\n",
    "convert_dtype(user, list(user.columns), dtype='float')\n",
    "convert_dtype(user, ['user_id'], dtype='object')\n",
    "df_name_and_variable_sizes(user, ['user_id'])     # print counts of unique user_id and unique session_id\n",
    "# df name is user ==> 5998 user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preferred perk customer index\n",
    "ppc = user[['monetization_rate', 'trips', \n",
    "            'days_cut_off_vs_sign_up', \n",
    "            'travellers_per_trip', \n",
    "              'tcv_per_traveller', \n",
    "              'tcv_per_room_unit', \n",
    "              'avg_tcv_per_seat_per_km']]\n",
    "ppc_corr_matrix = ppc.corr(numeric_only=True)\n",
    "ppc_corr_matrix.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\ppc_corr_matrix.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free hotel meal index\n",
    "fhm = user[['hotel_guests_per_trip', \n",
    "            'quality_travel_ratio', \n",
    "            'long_stay_ratio', \n",
    "            'staycation_ratio', \n",
    "            'foreign_trip_ratio', \n",
    "            'long_haul_ratio']]\n",
    "fhm_corr_matrix = fhm.corr(numeric_only=True)\n",
    "fhm_corr_matrix.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\fhm_corr_matrix.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free hotel night index\n",
    "fhn = user[['seats_per_trip', \n",
    "            'flight_only_ratio', \n",
    "            'premium_flight_ratio', \n",
    "              'extended_return_trip_ratio', \n",
    "              'foreign_trip_ratio', \n",
    "              'long_haul_ratio']]\n",
    "fhn_corr_matrix = fhn.corr(numeric_only=True)\n",
    "fhn_corr_matrix.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\fhn_corr_matrix.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free checked bag index\n",
    "fcb = user[['seats_per_trip', \n",
    "            'max_bags_per_seat', \n",
    "            'budget_flight_ratio', \n",
    "            'extended_return_trip_ratio', \n",
    "            'foreign_trip_ratio', \n",
    "            'long_haul_ratio']]\n",
    "fcb_corr_matrix = fcb.corr(numeric_only=True)\n",
    "fcb_corr_matrix.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\fcb_corr_matrix.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no cancellation fees index\n",
    "ncf = user[['conversion_rate', \n",
    "    'cancellations', 'cancel_rate', \n",
    "    'late_booking_ratio']]\n",
    "ncf_corr_matrix = ncf.corr(numeric_only=True)\n",
    "ncf_corr_matrix.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\ncf_corr_matrix.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusive discounts index\n",
    "edc = user[['clicks_per_session', 'browsing_mins_per_session', \n",
    "            'slippage_rate', \n",
    "            'avg_comparable_trip_discount', 'discount_trip_ratio', \n",
    "            'avg_days_pre_travel',\n",
    "              'late_booking_ratio']]\n",
    "edc_corr_matrix = edc.corr(numeric_only=True)\n",
    "edc_corr_matrix.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\edc_corr_matrix.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Pearson correlation between all possible pairs of columns\n",
    "corr_matrix = user.corr(numeric_only=True)\n",
    "# Export for future use\n",
    "corr_matrix.to_csv(CONFIG_PATH[\"corr_matrix\"], index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate matrix of number of unique user_ids within the number stdev of values in each column\n",
    "sample_size_matrix = sample_size_matrix(user, reference_col='user_id', threshold=2)\n",
    "# Export for future use\n",
    "sample_size_matrix.to_csv(CONFIG_PATH[\"sample_size_matrix\"], index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# G. Scale sub-metrics, calculate perk-based behavioral metric and run K-Means\n",
    "* Generate new df with perk allocation and K-Means clusters (selection df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Express lane: Import user df if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df name is user ==> 5998 user_id\n"
     ]
    }
   ],
   "source": [
    "user = pd.read_csv(CONFIG_PATH[\"user\"])                        # import data df if available\n",
    "convert_dtype(user, list(user.columns), dtype='float')\n",
    "convert_dtype(user, ['user_id'], dtype='object')\n",
    "df_name_and_variable_sizes(user, ['user_id'])     # print counts of unique user_id and unique session_id\n",
    "# df name is user ==> 5998 user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set up perk dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "perk_dict = {\n",
    "    'PPC_Index': {\n",
    "        \"Valued customer\": {\n",
    "            'monetization_rate': 0.083333333,\n",
    "            'trips': 0.083333333,\n",
    "            'days_cut_off_vs_sign_up': 0.166666667,\n",
    "            'travellers_per_trip': 0.166666667,\n",
    "            'tcv_per_traveller': 0.166666667,\n",
    "            'tcv_per_room_unit': 0.166666667,\n",
    "            'avg_tcv_per_seat_per_km': 0.166666667\n",
    "        }\n",
    "    },\n",
    "    'FHM_Index': {\n",
    "        \"Free hotel meal\": {\n",
    "            'hotel_guests_per_trip': 0.16666666666666666,\n",
    "            'quality_travel_ratio': 0.16666666666666666,\n",
    "            'long_stay_ratio': 0.16666666666666666,\n",
    "            'staycation_ratio': 0.16666666666666666,\n",
    "            'foreign_trip_ratio': 0.16666666666666666,\n",
    "            'long_haul_ratio': 0.16666666666666666\n",
    "        }\n",
    "    },\n",
    "    'FHN_Index': {\n",
    "        \"Free hotel night\": {\n",
    "            'seats_per_trip': 0.16666666666666666,\n",
    "            'flight_only_ratio': 0.16666666666666666,\n",
    "            'premium_flight_ratio': 0.16666666666666666,\n",
    "            'extended_return_trip_ratio': 0.16666666666666666,\n",
    "            'foreign_trip_ratio': 0.16666666666666666,\n",
    "            'long_haul_ratio': 0.16666666666666666\n",
    "        }\n",
    "    },\n",
    "    'FCB_Index': {\n",
    "        \"Free checked bag\": {\n",
    "            'seats_per_trip': 0.16666666666666666,\n",
    "            'max_bags_per_seat': 0.16666666666666666,\n",
    "            'budget_flight_ratio': 0.16666666666666666,\n",
    "            'extended_return_trip_ratio': 0.16666666666666666,\n",
    "            'foreign_trip_ratio': 0.16666666666666666,\n",
    "            'long_haul_ratio': 0.16666666666666666\n",
    "        }\n",
    "    },\n",
    "    'NCF_Index': {\n",
    "        \"No cancellation fees\": {\n",
    "            'cancellations': 0.166666666666667,\n",
    "            'cancel_rate': 0.166666666666667,\n",
    "            'conversion_rate': 0.333333333333333,\n",
    "            'late_booking_ratio': 0.333333333333333\n",
    "        }\n",
    "    },\n",
    "    'EDC_Index': {\n",
    "        \"Exclusive discounts\": {\n",
    "            'clicks_per_session': 0.083333333,\n",
    "            'browsing_mins_per_session': 0.083333333,\n",
    "            'slippage_rate': 0.166666667,\n",
    "            'avg_comparable_trip_discount': 0.166666667,\n",
    "            'discount_trip_ratio': 0.166666667,\n",
    "            'avg_days_pre_travel': 0.166666667,\n",
    "            'late_booking_ratio': 0.166666667, \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert perk_dict to perk df\n",
    "flat_list = []\n",
    "# Iterate through the nested dictionary and flatten it\n",
    "for index, metrics in perk_dict.items():\n",
    "    for desc, metric_weight_dict in metrics.items():\n",
    "        for metric, weight in metric_weight_dict.items():\n",
    "            flat_list.append([index, desc, metric, weight])\n",
    "# Create the DataFrame\n",
    "perk = pd.DataFrame(flat_list, columns=['index', 'perk', 'metric', 'weight'])\n",
    "name = next((x for x in globals() if globals()[x] is perk), None)\n",
    "perk.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "perk_cluster = perk[['index', 'perk']].drop_duplicates(keep='first').reset_index(drop=True)\n",
    "perk_cluster.to_csv(r\"C:\\\\data\\\\tt\\\\py\\\\perk_cluster.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preselection = user.copy() # initialize df to add index scores, use until the penultimate step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Boost high value customers for premium perks - PPC/VIP Index: Preferred perk customer index\n",
    "* Valued customers are cut off at 80th-percentile or higher in terms of their consolidated PPC score on monetization rates, seasoning on platform, bulk booking, unit value sold and spend $ related metrics\n",
    "* Favor valued customers by giving bonus points to boost their scores for exquisite perks i.e. free hotel meal or free hotel night\n",
    "* Boost up to an extra 0.50 points to the raw premium perk scores of valued customers, prior to final renormalization by min-max scaling\n",
    "* Composition metrics: 'monetization_rate', 'trips', 'days_cut_off_vs_sign_up', 'travellers_per_trip', 'tcv_per_traveller', 'tcv_per_room_unit', 'avg_tcv_per_seat_per_km'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPC_Index: ['monetization_rate', 'trips', 'days_cut_off_vs_sign_up', 'travellers_per_trip', 'tcv_per_traveller', 'tcv_per_room_unit', 'avg_tcv_per_seat_per_km']\n",
      "Scaling column monetization_rate:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.67. Mean=0.30, Std=0.19\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.44, Std=0.27\n",
      "\n",
      "Scaling column trips:\n",
      "- Initial: Actual Min=0.00, Max=8.00. Capped Min=0.00, Max=5.33. Mean=2.38, Std=1.47\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.44, Std=0.27\n",
      "\n",
      "Scaling column days_cut_off_vs_sign_up:\n",
      "- Initial: Actual Min=72.00, Max=737.00. Capped Min=119.18, Max=260.62. Mean=189.90, Std=35.36\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.49, Std=0.18\n",
      "\n",
      "Scaling column travellers_per_trip:\n",
      "- Initial: Actual Min=0.00, Max=3.00. Capped Min=0.15, Max=1.90. Mean=1.03, Std=0.44\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.50, Std=0.22\n",
      "\n",
      "Scaling column tcv_per_traveller:\n",
      "- Initial: Actual Min=0.00, Max=7065.36. Capped Min=0.00, Max=2021.80. Mean=792.55, Std=614.63\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.38, Std=0.24\n",
      "\n",
      "Scaling column tcv_per_room_unit:\n",
      "- Initial: Actual Min=0.00, Max=13359.00. Capped Min=0.00, Max=1045.96. Mean=299.12, Std=373.42\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.27, Std=0.21\n",
      "\n",
      "Scaling column avg_tcv_per_seat_per_km:\n",
      "- Initial: Actual Min=0.00, Max=0.39. Capped Min=0.00, Max=0.31. Mean=0.15, Std=0.08\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.47, Std=0.26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choice_ppc = list(perk_dict.keys())[0]\n",
    "# min-max scale relevant metrics and compute raw score\n",
    "output = compute_index_scores(df=preselection, index_weight_dict=perk_dict, \n",
    "                                 selected_index=choice_ppc, # edit here\n",
    "                                 col_filter=None, debug=True, \n",
    "                                 cap=True, threshold=2)\n",
    "output['user_id'] = preselection['user_id']\n",
    "cols = ['user_id'] + [col for col in output if col != 'user_id']\n",
    "output = output[cols]\n",
    "output.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\{choice_ppc}.csv\", index=False)\n",
    "preselection[choice_ppc] = output[choice_ppc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bonus point for high value customers for their premium perk scores \n",
    "pp_threshold = preselection[choice_ppc].quantile(0.80) # impose 80% percentile threshold, i.e. top 20% users by PPC Index score\n",
    "max_bonus_pt = 0.50\n",
    "preselection['bonus_pt'] = np.where(preselection[choice_ppc] >= pp_threshold,\n",
    "                                 np.maximum(((preselection[choice_ppc] - pp_threshold).div((1 - pp_threshold))) * max_bonus_pt, (max_bonus_pt * 0.5)),\n",
    "                                 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Exquisite perks\n",
    "* Top PPC scorers eligible for boost to scores\n",
    "* Aim to entrench existing behavior primarily "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FHM Index: Free hotel meal index\n",
    "* Incentive for customers who bulk book for quality travel experience (premium airline or luxury hotel). Favor staycationers as well as those on long haul and extended overseas trips\n",
    "* Composition metrics: 'hotel_guests_per_trip', 'quality_travel_ratio', 'long_stay_ratio', 'staycation_ratio', 'foreign_trip_ratio', 'long_haul_ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FHM_Index: ['hotel_guests_per_trip', 'quality_travel_ratio', 'long_stay_ratio', 'staycation_ratio', 'foreign_trip_ratio', 'long_haul_ratio']\n",
      "Scaling column hotel_guests_per_trip:\n",
      "- Initial: Actual Min=0.00, Max=4.00. Capped Min=0.07, Max=1.94. Mean=1.00, Std=0.47\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.50, Std=0.23\n",
      "\n",
      "Scaling column quality_travel_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.01, Max=1.00. Mean=0.74, Std=0.36\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.74, Std=0.36\n",
      "\n",
      "Scaling column long_stay_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.61. Mean=0.11, Std=0.25\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.16, Std=0.31\n",
      "\n",
      "Scaling column staycation_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.13. Mean=0.01, Std=0.06\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.01, Std=0.11\n",
      "\n",
      "Scaling column foreign_trip_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.13. Mean=0.01, Std=0.06\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.02, Std=0.12\n",
      "\n",
      "Scaling column long_haul_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.35. Mean=0.04, Std=0.16\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.07, Std=0.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choice_fhm = list(perk_dict.keys())[1]\n",
    "# min-max scale relevant metrics and compute raw score\n",
    "output = compute_index_scores(df=preselection, index_weight_dict=perk_dict, \n",
    "                                 selected_index=choice_fhm, # edit here \n",
    "                                 col_filter=None, debug=True, \n",
    "                                 cap=True, threshold=2)\n",
    "\n",
    "output['user_id'] = preselection['user_id']\n",
    "cols = ['user_id'] + [col for col in output if col != 'user_id']\n",
    "output = output[cols]\n",
    "output.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\{choice_fhm}.csv\", index=False)\n",
    "preselection[choice_fhm] = (output[choice_fhm] + preselection['bonus_pt']).clip(upper=1) # use clip to cap value at 1\n",
    "preselection[choice_fhm+'_original'] = output[choice_fhm]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FHN Index: One night free hotel with flight index\n",
    "* Incentive for customers who bulk book but more more exclusively just flights, with a bias for premium airline on long haul and extended overseas trips\n",
    "* Unexpectedness of reward may encourage future bookings with hotel as a plus to existing behavior  \n",
    "* Composition metrics: 'seats_per_trip', 'flight_only_ratio', 'premium_flight_ratio', 'extended_return_trip_ratio', 'foreign_trip_ratio', 'long_haul_ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FHN_Index: ['seats_per_trip', 'flight_only_ratio', 'premium_flight_ratio', 'extended_return_trip_ratio', 'foreign_trip_ratio', 'long_haul_ratio']\n",
      "Scaling column seats_per_trip:\n",
      "- Initial: Actual Min=0.00, Max=3.00. Capped Min=0.00, Max=1.86. Mean=0.91, Std=0.47\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.49, Std=0.25\n",
      "\n",
      "Scaling column flight_only_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.50. Mean=0.09, Std=0.20\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.16, Std=0.32\n",
      "\n",
      "Scaling column premium_flight_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=1.00. Mean=0.57, Std=0.41\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.57, Std=0.41\n",
      "\n",
      "Scaling column extended_return_trip_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.51. Mean=0.09, Std=0.21\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.14, Std=0.31\n",
      "\n",
      "Scaling column foreign_trip_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.13. Mean=0.01, Std=0.06\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.02, Std=0.12\n",
      "\n",
      "Scaling column long_haul_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.35. Mean=0.04, Std=0.16\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.07, Std=0.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choice_fhn = list(perk_dict.keys())[2]\n",
    "# min-max scale relevant metrics and compute raw score\n",
    "output = compute_index_scores(df=preselection, index_weight_dict=perk_dict, \n",
    "                                 selected_index=choice_fhn, # edit here \n",
    "                                 col_filter=None, debug=True, \n",
    "                                 cap=True, threshold=2)\n",
    "\n",
    "output['user_id'] = preselection['user_id']\n",
    "cols = ['user_id'] + [col for col in output if col != 'user_id']\n",
    "output = output[cols]\n",
    "output.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\{choice_fhn}.csv\", index=False)\n",
    "preselection[choice_fhn] = (output[choice_fhn] + preselection['bonus_pt']).clip(upper=1)\n",
    "preselection[choice_fhn+'_original'] = output[choice_fhn]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Standard perks\n",
    "* Favor less valued customers to incentivize better conversions for monetization (i.e. non-cancelled trips) by offering more flexibility with no cancellation fees or economic benefits like free checked bag and exclusive discounts\n",
    "* Cheaper as do not reduce potential units sold\n",
    "* Market efficiency by moving blocks of otherwise unsold and expiring units for perks with 'late_booking_ratio' metric component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* FCB Index: Free checked bag index\n",
    "* Incentive for customers who bulk book flights for long haul and extended overseas trips which drive need for more baggage allowance, with a bias for budget airline or those with more than 1 checked bags per seat\n",
    "* Composition metrics: 'seats_per_trip', 'max_bags_per_seat', 'budget_flight_ratio', 'extended_return_trip_ratio', 'foreign_trip_ratio', 'long_haul_ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FCB_Index: ['seats_per_trip', 'max_bags_per_seat', 'budget_flight_ratio', 'extended_return_trip_ratio', 'foreign_trip_ratio', 'long_haul_ratio']\n",
      "Scaling column seats_per_trip:\n",
      "- Initial: Actual Min=0.00, Max=3.00. Capped Min=0.00, Max=1.86. Mean=0.91, Std=0.47\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.49, Std=0.25\n",
      "\n",
      "Scaling column max_bags_per_seat:\n",
      "- Initial: Actual Min=0.00, Max=5.00. Capped Min=0.00, Max=1.92. Mean=0.69, Std=0.61\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.35, Std=0.30\n",
      "\n",
      "Scaling column budget_flight_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.91. Mean=0.25, Std=0.33\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.26, Std=0.34\n",
      "\n",
      "Scaling column extended_return_trip_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.51. Mean=0.09, Std=0.21\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.14, Std=0.31\n",
      "\n",
      "Scaling column foreign_trip_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.13. Mean=0.01, Std=0.06\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.02, Std=0.12\n",
      "\n",
      "Scaling column long_haul_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.35. Mean=0.04, Std=0.16\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.07, Std=0.24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choice_fcb = list(perk_dict.keys())[3]\n",
    "# min-max scale relevant metrics and compute raw score\n",
    "output = compute_index_scores(df=preselection, index_weight_dict=perk_dict, \n",
    "                                 selected_index=choice_fcb, # edit here \n",
    "                                 col_filter=None, debug=True, \n",
    "                                 cap=True, threshold=2)\n",
    "\n",
    "output['user_id'] = preselection['user_id']\n",
    "cols = ['user_id'] + [col for col in output if col != 'user_id']\n",
    "output = output[cols]\n",
    "output.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\{choice_fcb}.csv\", index=False)\n",
    "preselection[choice_fcb] = output[choice_fcb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NCF/FTC Index: No cancellation fees index (now Free to cancel)\n",
    "* Incentive for customers with higher conversion rate from session to booking but reduced monetized trips due to cancellations, and those who tend to book late (e.g. less than a week prior to travel)\n",
    "* May attact finnicky customers or business travellers who may be less price sensitive to purchase unsold units that risk expiring worthless (inventory obselence)\n",
    "* Composition metrics: 'cancellations', 'cancel_rate', 'conversion_rate', 'late_booking_ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCF_Index: ['cancellations', 'cancel_rate', 'conversion_rate', 'late_booking_ratio']\n",
      "Scaling column cancellations:\n",
      "- Initial: Actual Min=0.00, Max=2.00. Capped Min=0.00, Max=0.71. Mean=0.10, Std=0.31\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.10, Std=0.29\n",
      "\n",
      "Scaling column cancel_rate:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.37. Mean=0.04, Std=0.16\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.08, Std=0.26\n",
      "\n",
      "Scaling column conversion_rate:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.68. Mean=0.31, Std=0.18\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.46, Std=0.26\n",
      "\n",
      "Scaling column late_booking_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=1.00. Mean=0.42, Std=0.36\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.42, Std=0.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choice_ncf = list(perk_dict.keys())[4]\n",
    "# min-max scale relevant metrics and compute raw score\n",
    "output = compute_index_scores(df=preselection, index_weight_dict=perk_dict, \n",
    "                                 selected_index=choice_ncf, # edit here \n",
    "                                 col_filter=None, debug=True, \n",
    "                                 cap=True, threshold=2)\n",
    "\n",
    "output['user_id'] = preselection['user_id']\n",
    "cols = ['user_id'] + [col for col in output if col != 'user_id']\n",
    "output = output[cols]\n",
    "output.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\{choice_ncf}.csv\", index=False)\n",
    "preselection[choice_ncf] = output[choice_ncf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EDC/SOT Index: Exclusive discounts index (now Special offer travel)\n",
    "* Incentive for customers who exhibit bargain hunting behavior such spending extended time clicking and browsing but with high slippage rate (i.e. searches that fail to lead to bookings) and evidence of meticulous frugality such as booking as many days in advance and at prices below the average (for similar travel location and travel period)\n",
    "* Customers who tend to book late (within 7 days or less to date of travel) but otherwise show bargain hunting behavior also favored as means to shift units that risk expiring worthless with exclusive discounts\n",
    "* Composition metrics: 'clicks_per_session', 'browsing_mins_per_session', 'slippage_rate', 'avg_comparable_trip_discount', 'discount_trip_ratio', 'avg_days_pre_travel', 'late_booking_ratio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDC_Index: ['clicks_per_session', 'browsing_mins_per_session', 'slippage_rate', 'avg_comparable_trip_discount', 'discount_trip_ratio', 'avg_days_pre_travel', 'late_booking_ratio']\n",
      "Scaling column clicks_per_session:\n",
      "- Initial: Actual Min=4.12, Max=84.12. Capped Min=4.79, Max=26.71. Mean=15.75, Std=5.48\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.49, Std=0.22\n",
      "\n",
      "Scaling column browsing_mins_per_session:\n",
      "- Initial: Actual Min=0.53, Max=10.42. Capped Min=0.60, Max=3.31. Mean=1.95, Std=0.68\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.49, Std=0.22\n",
      "\n",
      "Scaling column slippage_rate:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.32, Max=1.00. Mean=0.69, Std=0.18\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.54, Std=0.26\n",
      "\n",
      "Scaling column avg_comparable_trip_discount:\n",
      "- Initial: Actual Min=-9.88, Max=0.99. Capped Min=-1.17, Max=0.99. Mean=0.10, Std=0.64\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.61, Std=0.21\n",
      "\n",
      "Scaling column discount_trip_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=0.86. Mean=0.24, Std=0.31\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.27, Std=0.33\n",
      "\n",
      "Scaling column avg_days_pre_travel:\n",
      "- Initial: Actual Min=0.00, Max=365.00. Capped Min=0.00, Max=55.09. Mean=10.44, Std=22.33\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.16, Std=0.15\n",
      "\n",
      "Scaling column late_booking_ratio:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=1.00. Mean=0.42, Std=0.36\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.42, Std=0.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choice_edc = list(perk_dict.keys())[5]\n",
    "# min-max scale relevant metrics and compute raw score\n",
    "output = compute_index_scores(df=preselection, index_weight_dict=perk_dict, \n",
    "                                 selected_index=choice_edc, # edit here \n",
    "                                 col_filter=None, debug=True, \n",
    "                                 cap=True, threshold=2)\n",
    "\n",
    "output['user_id'] = preselection['user_id']\n",
    "cols = ['user_id'] + [col for col in output if col != 'user_id']\n",
    "output = output[cols]\n",
    "output.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\{choice_edc}.csv\", index=False)\n",
    "preselection[choice_edc] = output[choice_edc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. MinMax Index level scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling column FHM_Index:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=1.00. Mean=0.30, Std=0.18\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.30, Std=0.18\n",
      "\n",
      "Scaling column FHN_Index:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=1.00. Mean=0.29, Std=0.21\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.29, Std=0.21\n",
      "\n",
      "Scaling column FCB_Index:\n",
      "- Initial: Actual Min=0.00, Max=0.88. Capped Min=0.00, Max=0.88. Mean=0.22, Std=0.15\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.25, Std=0.17\n",
      "\n",
      "Scaling column NCF_Index:\n",
      "- Initial: Actual Min=0.00, Max=0.94. Capped Min=0.00, Max=0.94. Mean=0.32, Std=0.19\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.34, Std=0.20\n",
      "\n",
      "Scaling column EDC_Index:\n",
      "- Initial: Actual Min=0.17, Max=0.78. Capped Min=0.17, Max=0.78. Mean=0.41, Std=0.10\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.40, Std=0.16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run final minmax scaling on perk scores (stored in preselection df) and generate a dict and its df\n",
    "list_to_minmax = [choice_fhm, choice_fhn, choice_fcb, choice_ncf, choice_edc] # list of index values\n",
    "prelim_scores = preselection[list_to_minmax] # df for minmax\n",
    "final_scaled_dict = minmax(prelim_scores, col_list=list_to_minmax, \n",
    "                           col_filter=None, debug=True, \n",
    "                           cap=False, threshold=None) # no need to cap the min and max at 2 stdev\n",
    "final_scaled = pd.DataFrame(final_scaled_dict)                          # to use for finalization later\n",
    "final_scaled['user_id'] = preselection['user_id']\n",
    "cols = ['user_id'] + [col for col in final_scaled if col != 'user_id']\n",
    "final_scaled = final_scaled[cols]\n",
    "final_scaled.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\final_scaled.csv\", index=False) # export for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling column FHM_Index_original:\n",
      "- Initial: Actual Min=0.00, Max=0.96. Capped Min=0.00, Max=0.96. Mean=0.25, Std=0.13\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.26, Std=0.13\n",
      "\n",
      "Scaling column FHN_Index_original:\n",
      "- Initial: Actual Min=0.00, Max=1.00. Capped Min=0.00, Max=1.00. Mean=0.24, Std=0.16\n",
      "- Scaled: Final Min=0.00, Max=1.00. Mean=0.24, Std=0.16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run minmax on the original scores stored in preselection df and generate a dict\n",
    "orig_to_minmax = [choice_fhm+'_original', choice_fhn+'_original'] # list of index values\n",
    "orig_prelim_scores = preselection[orig_to_minmax] # df for minmax\n",
    "orig_scaled_dict = minmax(orig_prelim_scores, col_list=orig_to_minmax, \n",
    "                           col_filter=None, debug=True, \n",
    "                           cap=False, threshold=None) # no need to cap the min and max at 2 stdev\n",
    "\n",
    "orig_scaled = pd.DataFrame(orig_scaled_dict)                          # to use for finalization later\n",
    "orig_scaled['user_id'] = preselection['user_id']\n",
    "cols = ['user_id'] + [col for col in orig_scaled if col != 'user_id']\n",
    "orig_scaled = orig_scaled[cols]\n",
    "orig_scaled.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\orig_scaled.csv\", index=False) # export for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-Means on transaction level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Local\\Temp\\ipykernel_43012\\3397598374.py:1: DtypeWarning: Columns (12,13,14,15,16,17,18,19,20,21,22,23,24,26,27,29,30,31,32,33,34,35,36,37,38,39,40,41,43,44,49,50,51,52,53,54,55,56,57,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  clean = pd.read_csv(CONFIG_PATH[\"clean\"])                        # import data df if available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df name is clean ==> 5998 user_id , 47980 session_id\n"
     ]
    }
   ],
   "source": [
    "clean = pd.read_csv(CONFIG_PATH[\"clean\"])                        # import data df if available\n",
    "clean_dtype_dict = data_dtype_dict\n",
    "convert_dtype_multi(clean, dtype_dict=clean_dtype_dict)          # clean and data df have same columns hence same columns need prepping for correct dtype\n",
    "df_name_and_variable_sizes(clean, ['user_id', 'session_id'])     # print counts of unique user_id and unique session_id\n",
    "# df name is clean ==> 5998 user_id , 47181 session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_t_var = [\n",
    "    'user_id', 'birthdate', 'days_cut_off_vs_sign_up', 'home_airport_lat', 'home_airport_lon', \n",
    "    'sign_up_date', 'session_start', 'session_end', 'flight_discount_amount',\n",
    "    'hotel_discount_amount', 'page_clicks', 'seats', 'departure_time', \n",
    "    'return_time', 'checked_bags', 'destination_airport_lat', 'destination_airport_lon', \n",
    "    'base_fare_usd', 'nights', 'rooms', 'check_in_time', 'check_out_time', \n",
    "    'hotel_per_room_usd', 'use_trip_id', 'distance_km', 'booking', 'cancelled_booking', \n",
    "    'trip', 'trip_with_flight', 'trip_with_hotel', 'combo_trip', 'flight_only_trip', \n",
    "    'hotel_only_trip', 'return_trip', 'foreign_trip', 'long_haul_trip', 'staycation', \n",
    "    'quality_travel', 'discount_trip', 'session_duration_mins', 'travel_start', \n",
    "    'travel_end', 'travel_days', 'trip_booking_time', 'days_pre_travel', 'long_stay', \n",
    "    'long_travel', 'early_booking', 'late_booking', 'booking_60_to_180_days', \n",
    "    'num_travellers', 'bags_per_seat', \n",
    "    'guests_per_room', 'base_fare_usd_per_km', 'room_nights', 'value_per_traveller', \n",
    "    'value_per_seat_per_km', 'comparable_trip_discount_ratio'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_t = clean[kmeans_t_var].copy()                                  # initialize relevant kmeans df\n",
    "kmeans_t = user_cluster(kmeans_t, groupby_column='user_id',            # add kmeans cluster to the df\n",
    "                      n_clusters=5, random_state=42, na_as_zero=True, \n",
    "                      numeric_only=True, n_init=10)\n",
    "kmeans_t = kmeans_t.groupby('user_id')['cluster'].first().reset_index()     # reset index to be exported as table of user_id and corresponding cluster\n",
    "kmeans_t = kmeans_t.drop_duplicates(keep='first')                           # drop potential duplicated rows except first occurrence\n",
    "kmeans_t.rename(columns={'cluster': 'kmeans_t'}, inplace=True)              # rename col\n",
    "kmeans_t['kmeans_t'] = \"T\" + (kmeans_t['kmeans_t'] + 1).astype(str)         # rename values\n",
    "kmeans_t.to_csv(CONFIG_PATH[\"kmeans_t\"], index=False)                       # export for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-Means on user metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df name is user ==> 5998 user_id\n"
     ]
    }
   ],
   "source": [
    "user = pd.read_csv(CONFIG_PATH[\"user\"])                         # import data df if available\n",
    "convert_dtype(user, list(user.columns), dtype='float')\n",
    "convert_dtype(user, ['user_id'], dtype='object')\n",
    "df_name_and_variable_sizes(user, ['user_id'])                   # print counts of unique user_id and unique session_id\n",
    "# df name is user ==> 5998 user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = [\n",
    "    \"user_id\",\n",
    "    \"avg_comparable_trip_discount\",\n",
    "    \"avg_days_pre_travel\",\n",
    "    \"avg_tcv_per_seat_per_km\",\n",
    "    \"browsing_mins_per_session\",\n",
    "    \"budget_flight_ratio\",\n",
    "    \"cancel_rate\",\n",
    "    \"cancellations\",\n",
    "    \"clicks_per_session\",\n",
    "    \"conversion_rate\",\n",
    "    \"days_cut_off_vs_sign_up\",\n",
    "    \"discount_trip_ratio\",\n",
    "    \"extended_return_trip_ratio\",\n",
    "    \"flight_only_ratio\",\n",
    "    \"foreign_trip_ratio\",\n",
    "    \"hotel_guests_per_trip\",\n",
    "    \"late_booking_ratio\",\n",
    "    \"long_haul_ratio\",\n",
    "    \"long_stay_ratio\",\n",
    "    \"max_bags_per_seat\",\n",
    "    \"monetization_rate\",\n",
    "    \"premium_flight_ratio\",\n",
    "    \"quality_travel_ratio\",\n",
    "    \"seats_per_trip\",\n",
    "    \"slippage_rate\",\n",
    "    \"staycation_ratio\",\n",
    "    \"tcv_per_room_unit\",\n",
    "    \"tcv_per_traveller\",\n",
    "    \"travellers_per_trip\",\n",
    "    \"trips\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Master\\AppData\\Local\\Temp\\ipykernel_43012\\2652901594.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "kmeans_u = user[metrics_list]                                           # initialize relevant kmeans df\n",
    "kmeans_u = user_cluster(kmeans_u, groupby_column='user_id',             # add kmeans cluster to the df\n",
    "                      n_clusters=5, random_state=42, na_as_zero=True, \n",
    "                      numeric_only=True, n_init=10)\n",
    "kmeans_u = kmeans_u.groupby('user_id')['cluster'].first().reset_index() # reset index to be exported as table of user_id and corresponding cluster\n",
    "kmeans_u = kmeans_u.drop_duplicates(keep='first')                       # drop potential duplicated rows except first occurrence\n",
    "kmeans_u.rename(columns={'cluster': 'kmeans_u'}, inplace=True)          # rename col\n",
    "kmeans_u['kmeans_u'] = \"U\" + (kmeans_u['kmeans_u'] + 1).astype(str)         # rename values\n",
    "kmeans_u.to_csv(CONFIG_PATH[\"kmeans_u\"], index=False)                   # export for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Finalize selection df to allocate perks and add in kmeans clustering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate user and user derived dfs which have the same number and order of user_id\n",
    "selection = pd.DataFrame(pd.concat([user, preselection[[choice_ppc, 'bonus_pt']], orig_scaled[orig_to_minmax], final_scaled[list_to_minmax]], axis=1)) # initialize selection df\n",
    "cols_to_rank_check = list_to_minmax # keep same order of cols as final_scaled\n",
    "\n",
    "# calculate ranks for each col and store them as new columns with rank post-fix\n",
    "for col in cols_to_rank_check:\n",
    "    new_col_name = f\"{col}_rank\"\n",
    "    selection[new_col_name] = selection[col].rank(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate based on rank first and if fail then by score \n",
    "\n",
    "# list of rank cols to use to allocate\n",
    "rank_cols_to_allocate = ['FHM_Index_rank', 'FCB_Index_rank', 'FHN_Index_rank', 'NCF_Index_rank', 'EDC_Index_rank']\n",
    "\n",
    "# Primary allocation by rank\n",
    "selection['primary_allocation_index'] = selection[rank_cols_to_allocate].apply(\n",
    "    lambda row: row.idxmin().replace('_rank', '') if sum(row == row.min()) == 1 else \"allocate by score\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Secondary allocation by score (if unallocated by rank)\n",
    "selection['final_allocation_index'] = selection['primary_allocation_index']\n",
    "mask = selection['primary_allocation_index'] == \"allocate by score\"\n",
    "selection.loc[mask, 'final_allocation_index'] = selection.loc[mask, cols_to_rank_check].idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative - allocate based on score first and if fail then by rank\n",
    "\n",
    "# list of rank cols to use to allocate\n",
    "rank_cols_to_allocate = ['FHM_Index_rank', 'FCB_Index_rank', 'FHN_Index_rank', 'NCF_Index_rank', 'EDC_Index_rank']\n",
    "\n",
    "# primary allocation\n",
    "selection['primary_allocation_index'] = np.where(selection[cols_to_rank_check].apply(lambda x: sum(x == x.max()), axis=1)==1,   # number of cols with top score==1\n",
    "                                        selection[cols_to_rank_check].apply(lambda row: row.idxmax(), axis=1), \n",
    "                                        \"allocate by rank\")                                                                     # initial allocation\n",
    "# secondary allocation\n",
    "selection['final_allocation_index'] = np.where(\n",
    "    selection['primary_allocation_index'] == \"allocate by rank\",\n",
    "    selection[rank_cols_to_allocate].apply(lambda row: row.idxmin().replace('_rank', ''), axis=1),\n",
    "    selection['primary_allocation_index']) # final allocation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove primary allocation col and simplify col name for the final allocation\n",
    "selection['index'] = selection['final_allocation_index']\n",
    "selection.drop(columns=['primary_allocation_index', 'final_allocation_index'], axis=1, inplace=True)           # drop useless col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perk\n",
    "selection = selection.merge(\n",
    "    perk_cluster[['index', 'perk']],\n",
    "    on='index', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans cluster names to user_id\n",
    "selection = selection.merge(\n",
    "    kmeans_t[['user_id', 'kmeans_t']],\n",
    "    on='user_id', how='left')\n",
    "\n",
    "selection = selection.merge(\n",
    "    kmeans_u[['user_id', 'kmeans_u']],\n",
    "    on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection.to_csv(CONFIG_PATH[\"selection\"], index=False)           # export for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Set analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perk vs kmeans_t\n",
    "seed_1 = selection.groupby(['perk', 'kmeans_t'])['user_id'].nunique().reset_index()       # initialize a groupby\n",
    "matrix_1 = seed_1.pivot(index='kmeans_t', columns='perk', values='user_id').fillna(0).astype(int)\n",
    "custom_order = ['free_hotel_meal', 'free_checked_bag', 'free_hotel_night', \n",
    "                'no_cancellation_fees', 'exclusive_discounts']      # to reorder perk cols\n",
    "matrix_1 = matrix_1.reindex(columns=custom_order)                   # reorder cols\n",
    "matrix_1.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\set_1.csv\", index=True)\n",
    "\n",
    "# perk vs kmeans_u\n",
    "seed_2 = selection.groupby(['perk', 'kmeans_u'])['user_id'].nunique().reset_index()       # initialize a groupby\n",
    "matrix_2 = seed_2.pivot(index='kmeans_u', columns='perk', values='user_id').fillna(0).astype(int)\n",
    "matrix_2 = matrix_2.reindex(columns=custom_order)                   # reorder cols\n",
    "matrix_2.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\set_2.csv\", index=True)\n",
    "\n",
    "# kmeans_u vs kmeans_t\n",
    "seed_3 = selection.groupby(['kmeans_u', 'kmeans_t'])['user_id'].nunique().reset_index()           # initialize a groupby\n",
    "matrix_3 = seed_3.pivot(index='kmeans_t', columns='kmeans_u', values='user_id').fillna(0).astype(int)\n",
    "matrix_3.to_csv(f\"C:\\\\data\\\\tt\\\\py\\\\set_3.csv\", index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
